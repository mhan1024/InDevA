{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87d7df0-b95a-4801-a86c-fffd750d8d0e",
   "metadata": {},
   "source": [
    "The dataset will contain a particular product's background information, which can be obtained from the Components section of the Developer Portal, poorly written tutorials, annotations for correcting mistakes, and properly written tutorials. \n",
    "Then, each text component (page/document) will be pre-processed as a large string fed to the model. \n",
    "\n",
    "The pre-processing step includes using the Beautiful Soup library to parse and extract necessary blocks of text from each web page. There will be 2 functions (listed below) that will gather the text from the Components page and the tutorials. \n",
    "\n",
    "Birst will be the product selected to run a small version of InDevA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106ced2-f7fa-4032-b445-4908e3ff15c4",
   "metadata": {},
   "source": [
    "# Installing & importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1065095d-a8d4-40ae-80cc-1f606ab46dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mhan\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mhan\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\mhan\\anaconda\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mhan\\anaconda\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mhan\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mhan\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mhan\\anaconda\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\mhan\\anaconda\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mhan\\anaconda\\lib\\site-packages (4.43.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.7 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 711.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.5 MB 3.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/9.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.0/9.5 MB 8.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.5 MB 8.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.0/9.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/9.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.0/9.5 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.4/9.5 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.3/9.5 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.9/9.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.3/9.5 MB 8.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.6/9.5 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.8/9.5 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.4/9.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.7/9.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.5 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.2/9.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.5 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.1/9.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.43.3\n",
      "    Uninstalling transformers-4.43.3:\n",
      "      Successfully uninstalled transformers-4.43.3\n",
      "Successfully installed transformers-4.44.0\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\mhan\\anaconda\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: torch in c:\\users\\mhan\\anaconda\\lib\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mhan\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mhan\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\mhan\\anaconda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\mhan\\anaconda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\mhan\\anaconda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mhan\\anaconda\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mhan\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mhan\\anaconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\mhan\\anaconda\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\mhan\\anaconda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\mhan\\anaconda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\mhan\\anaconda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing Beautiful Soup library\n",
    "!pip install beautifulsoup4\n",
    "!pip install beautifulsoup4 requests\n",
    "\n",
    "!pip install pyarrow\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install transformers torch\n",
    "!pip install sentencepiece\n",
    "\n",
    "!pip install --upgrade pyarrow\n",
    "!pip install --upgrade datasets\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "!pip install transformers[torch]\n",
    "\n",
    "!pip install tensorboard\n",
    "\n",
    "!pip install tensorflow\n",
    "\n",
    "!pip install accelerate -U\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31da6e50-e4b5-4413-9db1-fbce3323dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\mhan\\anaconda\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mhan\\anaconda\\lib\\site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f8ccfa-3830-4aa0-8377-a16ed9c1a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\mhan\\anaconda\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\mhan\\anaconda\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mhan\\anaconda\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mhan\\anaconda\\lib\\site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhan\\anaconda\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6963754b-be0c-4e83-9106-a54c38f23e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\mhan\\anaconda\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\mhan\\anaconda\\lib\\site-packages (from vaderSentiment) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mhan\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144ce673-8bb7-4bca-94a6-9fc9d23b8b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tk in c:\\users\\mhan\\anaconda\\lib\\site-packages (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a9ee5e-12c3-4335-96fc-a23be51c1b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mhan\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Beautiful Soup library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# Import regex library\n",
    "import re\n",
    "# Import sentence tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# Import json library\n",
    "import json\n",
    "# Import libraries for model\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import datasets\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from evaluate import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import libraries for visualizing training process\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Sentiment analysis \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# User interface\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e3477-f317-4bb2-8e98-8526bd86e08d",
   "metadata": {},
   "source": [
    "# Data pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76c2245-c9d7-438f-ae6e-75462fcac432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the Components page\n",
    "def process_components(url):\n",
    "    # Result string that stores extracted paragraphs and tables \n",
    "    result = \"\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code == 200), otherwise print error message.\n",
    "    if r.status_code == 200:\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        # Get all the tags on the page\n",
    "        body = soup.find_all()\n",
    "\n",
    "        # Iterate through each tag that is stored in body\n",
    "        for tag in body:\n",
    "            \n",
    "            # Get the title of the application -> <h1>\n",
    "            if tag.name == 'h1':\n",
    "                title = tag.text.strip()\n",
    "                \n",
    "            # If the tag is a paragraph, then concat to the result string\n",
    "            elif tag.name == 'p':\n",
    "                result += tag.text\n",
    "                result += ' '\n",
    "\n",
    "            # If the tag is a table, then concat all the values in the table, except for headers\n",
    "            elif tag.name == 'table':\n",
    "                # Iterate through each row in the table\n",
    "                for row in tag.find_all('tr'):\n",
    "                    # Get all the columns in each row -> should be 2 columns, one for term and other for definition\n",
    "                    cols = row.find_all('td')\n",
    "                    # Stitch together the columns, so it follows this formatting -> vocab: definition\n",
    "                    text = cols[0].text.strip() + \": \" + cols[1].text.strip()\n",
    "\n",
    "                    if text[-1] != '.':\n",
    "                        text += '.'\n",
    "                    text += ' '\n",
    "                    \n",
    "                    # Concat text to result string\n",
    "                    result += text\n",
    "\n",
    "        # Return the result string as a json string\n",
    "        components_dict = {\n",
    "            'application_title': title,\n",
    "            'application_description': result,\n",
    "            'tutorials': []\n",
    "        }\n",
    "        \n",
    "        return json.dumps(components_dict)\n",
    "    else: \n",
    "        return { }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab7f7ee-5d4a-4975-a7da-d4c2446a0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the Tutorials page\n",
    "def process_tutorial(url):\n",
    "    # Result string that stores extracted paragraphs and tables \n",
    "    result = \"\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code == 200), otherwise print error message.\n",
    "    if r.status_code == 200:\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # print(soup.prettify())\n",
    "\n",
    "        # Set variables for getting the business problem\n",
    "        is_business_problem = False\n",
    "        business_problem = ''\n",
    "\n",
    "        # Set variables for getting the tutorial title and steps\n",
    "        is_tutorial = False        \n",
    "        tutorial_steps = ''\n",
    "        \n",
    "        # Set variables for getting requirements \n",
    "        is_requirements = False\n",
    "        requirements = []\n",
    "        \n",
    "        # Get all the tags on the page\n",
    "        body = soup.find_all()\n",
    "\n",
    "        for tag in body:\n",
    "            \n",
    "            # Get tutorial title\n",
    "            if tag.name == 'h1':\n",
    "                title = tag.text\n",
    "\n",
    "            # Business Problem and Tutorial sections have <h2> tag. So depending on the text associated with each tag, process accordingly. \n",
    "            elif tag.name == 'h2':\n",
    "\n",
    "                # Tag == Business Problem, then set status as true\n",
    "                if tag.text == 'Business Problem':\n",
    "                    is_business_problem = True\n",
    "\n",
    "                # Otherwise, set status as false and set the status for is_tutorial as true\n",
    "                else:\n",
    "                    is_business_problem = False\n",
    "                    is_tutorial = True\n",
    "\n",
    "            # Requirements and Components sections have <h3> tag. But only process the Requirements section and set status as true. \n",
    "            elif tag.name == 'h3':\n",
    "                \n",
    "                if tag.text == 'Requirements':\n",
    "                    is_requirements = True\n",
    "                else:\n",
    "                    is_requirements = False\n",
    "\n",
    "            elif tag.name == 'ul':\n",
    "\n",
    "                # Requirements list is before Tutorials, so is_tutorial status must be false.\n",
    "                if is_requirements and is_tutorial == False:\n",
    "                    # Each requirement is listed as a <li> element\n",
    "                    elements = tag.find_all('li')\n",
    "\n",
    "                    # Incorporate data cleaning and add to requirements list\n",
    "                    for e in elements:\n",
    "                        requirements.append(e.text.strip() + '.')\n",
    "\n",
    "                elif is_tutorial:\n",
    "                    elements = tag.find_all('li')\n",
    "\n",
    "                    for e in elements:\n",
    "                        text = e.text\n",
    "                            \n",
    "                        eop = re.search('Why OS|Terms|Privacy Policy', text)\n",
    "\n",
    "                        if not eop:\n",
    "                            \n",
    "                            if not text in tutorial_steps:\n",
    "                                if text[-1] != '.':\n",
    "                                    text += '. '  \n",
    "                                    \n",
    "                                tutorial_steps += text\n",
    "                                tutorial_steps += ' '\n",
    "\n",
    "            elif tag.name == 'ol':\n",
    "                \n",
    "                if is_tutorial:\n",
    "                    steps = tag.find_all('li')\n",
    "\n",
    "                    for s in steps:\n",
    "                        \n",
    "                        text = s.text\n",
    "                        \n",
    "                        if not text in tutorial_steps:\n",
    "                            tutorial_steps += text\n",
    "                            tutorial_steps += ' '\n",
    "                        \n",
    "            # When processing <p> tags, check statuses and store the text corresponding to whichever status is set to true\n",
    "            elif tag.name == 'p':\n",
    "                \n",
    "                if is_business_problem:\n",
    "                    business_problem += tag.text\n",
    "                    business_problem += ' '\n",
    "\n",
    "                elif is_tutorial:\n",
    "                    # The text that appears are all warning boxes\n",
    "                    text = tag.text\n",
    "                    # Don't add Difficulty and Estimated completion time\n",
    "                    diff = re.search('Difficulty:|Estimated Completion Time:', text)\n",
    "                    \n",
    "                    if not diff:\n",
    "                        if not text in tutorial_steps:\n",
    "                            tutorial_steps += text\n",
    "                            tutorial_steps += ' '\n",
    "\n",
    "        # Put together elements of json string\n",
    "        business_problem = business_problem.strip()\n",
    "        cleaned_tutorial_steps = sent_tokenize(tutorial_steps.strip())\n",
    "            \n",
    "        tutorial_dict = {\n",
    "            'tutorial_title': title,\n",
    "            'business_problem': business_problem,\n",
    "            'requirements': requirements,\n",
    "            'tutorial_steps': cleaned_tutorial_steps,\n",
    "            'feedback': 'None.'\n",
    "        }\n",
    "\n",
    "\n",
    "        return json.dumps(tutorial_dict)\n",
    "        \n",
    "    else: \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06519661-c4e2-46e1-9aaf-4e1c82cebb65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get all associated URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd2e125-2e7d-4e65-9006-3796a9c7b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the links listed on the tutorials page and returns a list of them\n",
    "def get_tutorial_urls(url):\n",
    "    \n",
    "    tutorials_url_lst = []\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code == 200), otherwise print error message.\n",
    "    if r.status_code == 200:\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # Get all the <a class=...> tags (tutorial links)\n",
    "        links = soup.find_all('a', class_='wp-block-pages-list__item__link')\n",
    "\n",
    "        # Store the href part of the tag\n",
    "        for l in links:\n",
    "            tutorials_url_lst.append(l['href'])\n",
    "\n",
    "    # Return the list\n",
    "    return tutorials_url_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7d7209d-f72e-492a-a0a1-87328ebe89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the urls that will be used in the dataset\n",
    "def get_urls(url_lst):\n",
    "    new_lst = []\n",
    "\n",
    "    # Go through each url in the list and check if it links to the Tutorials section \n",
    "    for u in url_lst:\n",
    "        is_tutorial = re.search(\"tutorials\", u)\n",
    "\n",
    "        # If the url contains 'tutorials' in its address, then get all the tutorial links and add to list\n",
    "        if is_tutorial:\n",
    "            new_lst = new_lst + get_tutorial_urls(u)\n",
    "\n",
    "        # Otherwise, just add the Components url\n",
    "        else:\n",
    "            new_lst.append(u)\n",
    "\n",
    "    # Return list\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e11ba8-a1f8-4f96-b108-b404af8a2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all the text from web pages to create the dataset.\n",
    "def get_dataset(url_lst):\n",
    "\n",
    "    # For every url in the list, check if it is a components or a tutorial page.\n",
    "    for u in url_lst:\n",
    "        \n",
    "        is_components = re.search(\"components\", u)\n",
    "        is_tutorial = re.search(\"tutorials\", u)\n",
    "\n",
    "        # If it is a component, then call process_components\n",
    "        if is_components:\n",
    "            result_json = json.loads(process_components(u))\n",
    "\n",
    "        # If it is a tutorial, then call process_tutorial\n",
    "        if is_tutorial:\n",
    "            tutorial_json = json.loads(process_tutorial(u))\n",
    "            result_json['tutorials'].append(tutorial_json)\n",
    "\n",
    "    return json.dumps(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e219c3c-a843-4fc2-acac-a1e4f36d7494",
   "metadata": {},
   "source": [
    "# Assembling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248b6e59-aee2-4713-853b-ba4c1d4f3108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"application_title\": \"Analytics\", \"application_description\": \"Infor Birst is an end-to-end, enterprise caliber analytics tool.  Birst delivers business intelligence and analytics dashboards to enable business decisions. Birst combines data from different sources in a single Networked BI platform. Reports; including charts, tables, geo maps, and key performance indicators; are organized into dashboards for presentation in a browser or a native mobile app. External content from other web pages can also enhance the end user experience. Birst is available as a multi-tenant cloud architecture or as an Appliance for in-house deployments. The Birst Networked BI platform combines the capability, scale, and data governance required for enterprise data with data from end users to create a blended data platform. Centralized and decentralized teams can leverage trusted corporate data along with local data for agile, robust business analytics. The Birst Connectivity Framework provides access to data in the enterprise, whether it is in an existing data warehouse, in flat or structured files, or in cloud business applications. Birst extracts data from those sources, then organizes it and models it into a user data store, making the data ready for analysis. Birst does this using Automated Data Refinement (ADR) and a transformation language (ETL) to automatically merge data and create the data store, unlike the lengthy and complex work needed to create a traditional data warehouse. The data store unifies your data from multiple sources, refines it for business use, and provides a consistent view of all data to all users. To augment the content in the user data store, Birst Live Access can directly query on-premise data sources in real time. Automated DataRefinement (ADR): Patented technology that automates the process of creating a robust dimensional model and semantic layer. Networked BI: Patented technology that brings both centralized and decentralized data together to provide governed data in an agile environment. Semantic Layer: A business representation of complex company data for end user consumption. Orchestration: Provides a module to create and manage workflows for data extraction, processing, and space operations. Enterprise Space: Enable complex, scalable data management with dimension data models. Professional Space: Relational data model with low code, now code self-service data preparation. Birst Query Language (BQL): A proprietary logical query language for defining expressions. Extract, Transform, Load (ETL): A process in database usage and especially in data warehousing thatinvolves extracting data from outside sources, transforming it to fit operational needs, and loading it into the target database or user data store (warehouse). Birst\\\\u00a0provides an integrated Networked BI platform for data-driven analysis.\\\\u00a0 Birst\\\\u00a0includes:  Birst Home: The Birst\\\\u00a0Home page is your starting point.It shows the spaces, the containers that organize and hold data, metadata, reports and dashboards. An account may have one space or use separate spaces with different sets of data for different analytical purposes.Home is the entry point for BI developers, data analysts, and administrators to work with data in\\\\u00a0Birst. Business user accounts can be configured to directly open dashboards and bypass Home. Dashboards 2.0: Dashboard authors create and view interactive dashboards that contain reports built in Designer and Visualizer. Business users can browse the dashboards, drill down into details, and share them via email. In addition to viewing dashboards in web browsers, end users can access them from\\\\u00a0Birst\\\\u00a0Mobile apps.The new generation of\\\\u00a0Birst\\\\u00a0Dashboards is based on responsive design principals and promotes highly interactive user experiences. Organize Visualizer and Designer reports onto dashboards, add powerful filters, key performance indicators (KPIs), geo maps, and text boxes, and group dashboards into collections. Visualizer: Business users and report writers access Visualizer to explore enterprise data and quickly get answers to business questions. \\\\u00a0Visualizer helps business users and report writers to rapidly answer questions and discover information. You don\\\\u2019t have to have formal expertise to interact with the drop-down menus and drag-and-drop columns. You can save and add reports to dashboards. Designer: Designer is a tool for Data analysts and report writers. It provides pixel-perfect layout capabilities and advanced functionality such as column selectors and drilling. Use Designer to develop highly formatted enterprise reports that you want to print, schedule for email delivery, or export. Admin: Developers, data analysts, and administrators use the Admin interface to create and manage the user data store, report catalogs, user accounts, and access permissions. The\\\\u00a0Admin module provides both data and system-related operations. Access to functionality is based on your user permissions.Data processed in Admin is available for analysis and reporting in Designer and Visualizer. Connectivity Network: Birst\\\\u00a0provides various ways of connecting to your existing data:Birst\\\\u00a0Connect\\\\u00a0is a Java Web Start application that automates uploading and processing data in\\\\u00a0Birst. It runs behind your firewall in order to access files and data sources at your location.\\\\u00a0Birst\\\\u00a0Connect uses HTTPS and supports JDBC data sources and SAP.Birst\\\\u00a0Live Access\\\\u00a0allows\\\\u00a0Birst\\\\u00a0to access data resident in local data marts and data warehouses without requiring that data to first be uploaded into\\\\u00a0Birst.Application Connectors\\\\u00a0provide access to many enterprise applications including Marketo and Salesforce.Upload Files\\\\u00a0such as Microsoft Excel, Access, or delimited ASCII text. There is a lot to learn in the Infor Platform. A quick reference sheet is always helpful. Check out the Analytics Cheat Sheet. Need information on a specific feature or function or a quick overview? These short videos may be just what you are looking for. Check out our Infor Analytics playlist on YouTube. Product documentation is the go-to reference for how specific parts of the product work. For online, searchable, easy to understand docs see Infor Birst documentation. Collaborating with others in your industry is a great way to learn and help others.  Start participating in the Infor Birst Community today! Infor Campus offers Learning Paths that combine video based and instructor-led training. If you are an Infor customer then check out courses on Infor U Campus. We recommend the following courses specifically for this component: \", \"tutorials\": [{\"tutorial_title\": \"Birst Cloud Agent Installation\", \"business_problem\": \"You wish to use cloud analytics for insights on your on-premise data but don\\\\u2019t know how to connect them. Birst Cloud Agent (Birst) simplifies the process of transferring, accessing, and integrating data into a cloud-based analytics platform. Not only is the process more secure, reliable, and efficient, but Birst also eliminates the need for manual data extraction and manipulation. Birst serves as the bridge between on-premises data sources and the Birst cloud environment, which ensures that your data is up-to-date and accessible for accurate reporting and analysis in real-time. This way, you and your organization can make data-driven decisions quicker, improve operational efficiency, and gain valuable insights from your data across different systems and databases.\", \"requirements\": [\"Internet Connection.\", \"Birst Analytics Platform Account Credentials.\", \"Birst Cloud Agent.\", \"Data Sources.\", \"Java 8 JDK or JRE.\"], \"tutorial_steps\": [\"Log into Birst and create a new space.\", \"To do so, click the \\\\u2018Create New Space\\\\u2019 hyperlink located in the middle of the Infor Birst welcome page.\", \"From the \\\\u2018Create New Space\\\\u2019 menu, we can choose to create an Enterprise, Professional, or Usage Tracking space.\", \"Make sure to name your space appropriately and save when finished.\", \"Enterprise: the most advanced and comprehensive edition, which offers support for complex data transformations, advanced scheduling, and robust governance and security controls.\", \"This is most suitable for large organizations with complex data integration requirements and a demand for a high level of governance and security.\", \"Professional: the mid-tier edition, which offers a balanced set of features for data integration but has limitations to its capabilities compared to the Enterprise edition.\", \"This is most suitable for medium-sized organizations with moderate data integration.\", \"Usage Tracking: a specialized edition that focuses on tracking and monitoring data usage and access patterns.\", \"This edition provides analytics and reporting capabilities to gain insights into how the data is being used within Birst.\", \"You are able to monitor user activity, track report usage, and analyze data consumption patterns.\", \"This is most suitable for organizations that want to understand user engagement and optimize their data analytic processes.\", \"When the space is created, go to the \\\\u2018Modeler Connect\\\\u2019 page to create a connection to a data source.\", \"In the top left of the welcome page is the \\\\u2018Global Navigation Menu\\\\u2019 button, which is symbolized by 3 horizontal lines.\", \"Click the button to expand the menu, expand the \\\\u2018Modeler\\\\u2019 section, and the select \\\\u2018Connect\\\\u2019 tab.\", \"Depending on your data source, you can select between the SQL Databases option or the Files option.\", \"In this tutorial, we will be selecting the SQL Databases option.\", \"SQL Databases: well-suited for large volumes of structured and relational data with predefined schemas.\", \"Files: a more flexible option for CSV or Excel files and works best for a smaller volume of unstructured or semi-structured data.\", \"Once you have made your selection, select and assign an agent.\", \"Assigning an agent means you are telling Birst which agent is used to access a particular on-premise data source (or files).\", \"Your agent needs to have access to the data source we are connecting to, and it is very probable an organization would have more than one agent running in a Birst environment.\", \"If there are not any existing agents installed, simply select the \\\\u2018Click here\\\\u2019 hyperlink (located below the grayed out \\\\u2018Select agent\\\\u2019 button) to download the \\\\u2018BirstCloudAgent.zip\\\\u2019 installation file.\", \"Note: A Birst Agent is required when connecting to on-premise data sources.\", \"After the zip file has downloaded, extract its contents and make note of their location.Optional: you can add an identifier to the end of the folder name to specify which environment is being connected to with the current agent.\", \"In the sample video, we used the identifier \\\\u2018_EU\\\\u2019 after extracting its contents to C:\\\\\\\\BirstCloudAgent_EU.\", \"The agent can run in a batch mode or as a service.\", \"In this tutorial, the agent will be running in batch mode.\", \"If you want your agent to run as a service, refer to the instructions in the \\\\u2018Readme.txt\\\\u2019 file.\", \"Batch mode: the agent is executing tasks in a batch-like manner, either periodically or at scheduled intervals.\", \"This is most suitable for scenarios where data changes occur at regular intervals.\", \"Service: the agent remains active and ready to handle any data integration at any time, unlike batch mode.\", \"The agent is operating as a continuous background service on a server or machine and constantly monitors for changes without waiting for a trigger or schedule.\", \"This is most suitable for immediate or near-real-time data integration.\", \"In the contents of the unzipped download \\\\u2014 Locate the \\\\u2018Agent.bat\\\\u2019 file in the \\\\u2018bin\\\\u2019 folder.\", \"There are three options to run the agent:\\\\n\\\\nYou can double click the bat-file\\\\nYou can right-click the bat-file and chose Run as Administrator (some security setups on computers require this to run bat-files).\", \"Start the command prompt, navigate to the file, and run the file.\", \"This is recommended for the first time use, so you can see what is going on with the agent.\", \"After you see it run, you can use the above options.\", \"Warning: common errors can occur when Java is not installed, the incorrect version is installed, or Java home has not been set.\", \"Go back to the Infor Birst portal and click the \\\\u2018Select Agent\\\\u2019 button.\", \"In the agent selection menu, click the \\\\u2018Refresh\\\\u2019 button because we have installed a new agent.\", \"We only want to choose from the active agents only, so click the \\\\u2018Show online agents only\\\\u2019 button, which is located directly below the \\\\u2018Refresh\\\\u2019 button.\", \"The newly installed agent should be located at the top of the list.\", \"Select the check box next to the agent name to activate the agent.\"], \"feedback\": \"The steps are easy to follow and understand. You must capitalize the word\\'s first letter directly after a colon. The tone needs to remain formal. Refrain from using words, like we, ours, our, us, and let\\'s.\"}, {\"tutorial_title\": \"Changing visualizations based on user selection\", \"business_problem\": \"You have a dashboard that displays the correct information, but in some situations the user would like to analyze the data from different perspectives using different visualizations. You would like to give them a selection option on the dashboard to choose the visualization they desire. Analytics\", \"requirements\": [\"Edit Dashboard right (for dashboard filter editing).\", \"If Disable Filter Edit Access permission is enabled, the user can only create and edit Local Filters..\", \"If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the \\\\u2018M3A Administration\\\\u2018 role for Edit Dashboard access..\"], \"tutorial_steps\": [\"The\\\\u00a0Birst View Selector\\\\u00a0allows a user to switch between the original chart and a table view in a dashlet.\", \"Dashboard creator enables or disables the View Selector for each dashlet.\", \"Depending on how many measures and attributes your original chart uses, the View Selector will have a different number of chart options available.\", \"However if you want to include a map view, or to switch between reports and charts that hold a different data set, you need a different approach.\", \"By setting up a filter that controls which reports are visible you can give the user a way to easily switch between reports right on the dashboard.\", \"You can use \\\\u2018Data Driven Dashboards\\\\u2019 to overlay different dashlets on top of each other and show/hide them using a dashboard filter.\", \"Showing and hiding dashlets (reports, KPIs, images) enables the user to change the way they analyze data while staying on a single dashboard.\", \"Based on selections made on in the filters the reports can be shown or hidden, changing the design of the whole dashboard.\", \"Create one or more mandatory filters as parameters that will deliver specific values to the condition.\", \"In the video below we create a filter called \\\\u2018Report Selector\\\\u2019 that will display three options: Map, Table, and Column.\", \"Each option is mapped to it\\\\u2019s own value Map \\\\u2192 Map, Table \\\\u2192 Tab, Columns \\\\u2192 Col.\", \"These values are the output of the filter.\", \"We change the filter from \\\\u2018Multiple\\\\u2019 to \\\\u2018Single\\\\u2019 selection so that it produces only one result, and we set it to \\\\u2018Mandatory\\\\u2019 so that one of the options is always selected, even when the page loads.\", \"Create conditions that use the values from the parameter filters.\", \"\\\\u2018Data Driven Dashboards\\\\u2019 allow us to show and hide any dashlet on the dashboard based on a condition.\", \"In the video below we create a condition called \\\\u2018Show Table\\\\u2019, and we connect it to the filter we created earlier called \\\\u2018Report Selector\\\\u2019.\", \"The condition uses the output from the filter, and when that output is equal to \\\\u2018Tab\\\\u2019 then the condition is true.\", \"Add reports, KPIs or any dashlet to the dashboard, and associate the conditions to the dashlet by first clicking the condition, and then the dashlet.\", \"You will see a \\\\u2018hammer\\\\u2019 icon on the bottom right corner of the dashlet associated with the condition.\", \"As long as the condition is true the associated dashlet will be shown.\", \"Once the parameter filter is changed the condition will become false and the dashlet will hide.\", \"The user will make a selection on the embedded filter.\", \"Based on the users selection the filter will generate the corresponding parameter.\", \"And the condition will be either True or False.\", \"If the condition is true, the associated dashlets\\\\u00a0will be visible, otherwise they will remain hidden.\", \"How to set up Dashboard Filters  How to setup Data Driven Dashboards Basic Dashboard Filtering in the Birst How-to series on YouTube Advanced Dashboard Filtering in the Birst How-to series on YouTube\"], \"feedback\": \"To main a professional tone, refrain from using words, like us or we. Otherwise, steps are easy to follow and understand.\"}, {\"tutorial_title\": \"Dashboard filters of dynamically changing Measures and Dimensions\", \"business_problem\": \"You have a dashboard that communicates valuable information, but you want the same report to switch between different metrics or dimensions. For example, a pie chart where an employee can toggle between revenue per city, to revenue by country, or revenue by sales manager.  Another example would be to switch between multiple measures on a single report, such as from Revenue per country, to number of customers per country.\", \"requirements\": [\"Edit Dashboard\\\\u00a0right (for dashboard filter editing).\", \"If\\\\u00a0Disable Filter Edit Access\\\\u00a0permission is enabled, the user can only create and edit Local Filters..\", \"If you are using\\\\u00a0an Infor Cloudsuite, you\\\\u00a0can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the \\\\u2018M3A Administration\\\\u2018 role for Edit Dashboard access..\"], \"tutorial_steps\": [\"The video below shows you how to create a filter with parameters that use the measures.\", \"In Visualizer, Advanced Tools/BQL Editor, create a Saved Expression that will collect and interpret the values from the parameter.\", \"ex:\\\\u00a0<EVAL!GetPromptValue(\\\\u2018Value Selector\\\\u2019,\\'[OrderDate: # Distinct CustomerID]\\\\u2019)!> GetPromptValue\\\\u00a0\\\\u2013 captures the value provided by the parameter called\\\\u00a0Value Selector, as a string.\", \"If the parameter is empty, then a default string is used, in this case\\\\u00a0[OrderDate: # Distinct CustomerID].\", \"<EVAL!\", \"\\\\u2013 evaluates the string as a Measure or Dimension.\", \"3.\", \"Finally, use the newly created Saved Expression in your report as a regular Measure or Dimension.\", \"The user will make a selection on the embedded filter.\", \"Based on the users selection the filter will generate the corresponding parameter.\", \"The\\\\u00a0GetPromptValue\\\\u00a0function will capture the value from the parameter, and\\\\u00a0<EVAL!\", \"will convert the captured string to a Measure or Dimension.\", \"How to set up Dashboard Filters.\", \"Basic Dashboard Filtering in the Birst How-to series on YouTube Advanced Dashboard Filtering in the Birst How-to series on YouTube Alternate solution:\\\\u00a0Creating Column Selector\"], \"feedback\": \"To improve clarity, explain what the variables do, such as the \\'GetPromptValue\\' variable has a description to explain its function. Provide more information about the difference between a \\'measure\\' and a \\'dimension\\' and when to use them.\"}, {\"tutorial_title\": \"Dynamically change Time Series Types in Reports\", \"business_problem\": \"Users want to use different, non-standard time categories in their reports, and they prefer not having to use several reports or dashboard pages to do this. \\\\u00a0 Analytics\", \"requirements\": [\"The right to Edit Dashboards, use Visualizer, Saved Expressions..\", \"A properly modeled dataset, specifically that the Measure comes from a fact table with a date, and that the date is connected to the\\\\u00a0Time dimensions, in other words, that the \\\\u2018Analyze By Date\\\\u2019 is toggled on..\", \"If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the \\\\u2018M3A Administration\\\\u2018 role for Admin and Visualizer access..\"], \"tutorial_steps\": [\"Expected Completion Time: 10 minutes Infor Birst has a great ability to quickly compare data between different periods.\", \"All that\\\\u2019s needed is a measure, a date, and you can choose one of the following Time Series Types: When building reports, there is no way to create additional custom time categories, like Trailing Half Year, Trailing Four Months, or any Weekly Time Series.\", \"Also there could be customers that need to analyze data weekly, and while access to extra time categories can be enabled through support there is a way to develop these and more time categories in Visualizer.\", \"In situations like this we can use the power of the function RSUM and build a report that will show data based on a Time Series we define.\", \"In addition to time categories RSUM can calculate the trailing sum across any dimension, including text such as for example roman numerals I, II, III, IV.\", \"For the Trailing N Months, we can simply use Running Sum or RSUM.\", \"Birst has a built in Trailing 3 Months which we can replicate with a function:\\\\u00a0RSUM(window size,[ Measure]) where\\\\u00a0window size\\\\u00a0is the number of periods to consider, be it weeks, days or in this case, three months.\", \"Using the BQL Editor, we can create expressions.\", \"SavedExpression(\\\\u2018Custom T3M\\\\u2019) =\\\\u00a0RSUM(3,[OrderDate: # Distinct OrderID]).\", \"We can easily change this to be Trailing 4, 5, 6 or any number of periods by changing the window size.\", \"You can see in the above image, that 01/2021 column only includes the data for that month.\", \"This is controlled by the Filter/more options and can be changed.\", \"When \\\\u2018Use as a display filter\\\\u2019 is toggled off only the displayed data is used in the calculation.\", \"When \\\\u2018Use as a display filter\\\\u2019 is toggled on all data is used in the calculation, and then only the relevant data is displayed.\", \"Below is an image of the same report with \\\\u2018Use as a display filter\\\\u2019 toggled on.\", \"Note that the same\\\\u00a0RSUM(3,[Measure]) will show three months data if the Date Dimension is monthly, it will show three days data if the Date Dimension is daily, and it will show three years data if the Date Dimension is yearly.\", \"Furthermore, RSUM has no sense of time.\", \"So depending on the sorting used in the report, for January it can add Nov, Dec, and Jan when the months are sorted in descending order, but it will add Mar, Feb, Jan, if the order is ascending.\", \"While Infor Birst already offers Month Ago, with RSUM we can achieve 2 months ago or\\\\u00a0 5 months ago.\", \"The measurement for Month Ago is RSUM(2,[Measure]) \\\\u2013 [Measure].\", \"The measurement for 2 Months Ago is\\\\u00a0 RSUM(3,[Measure]) \\\\u2013 RSUM(2,[Measure]).\", \"The measurement for Quarter Ago is RSUM(4,[Measure]) \\\\u2013 RSUM(3,[Measure]).\", \"The measurement for Year Ago is RSUM(13,[Measure]) \\\\u2013 RSUM(12,[Measure]).\"], \"feedback\": \"Define \\'trailing half year, trailing four months\\' for better understanding. Define \\'Running Sum\\' and what it is used for. To main a professional tone, refrain from using words, like us or we.\"}, {\"tutorial_title\": \"Highlight the Performance on a Geographical Map\", \"business_problem\": \"You would like to create a dashboard of Sales Performance for different locations, highlighting the high-performing and low-performing locations on a map. A map visualization will occupy less space on dashboard while showing lot of information. Analytics To accomplish this task, we need to have:\", \"requirements\": [\"The right to create a Professional space.\", \"Birst Visualizer  access (for creating a report).\", \"Edit Dashboard right (for adding the report to the dashboard).\", \"If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the \\\\u2018M3A Administration\\\\u2018 role for Visualizer and Edit Dashboard access.\", \"Data containing a Geo Attribute, i.e. Country, City, State (for USA), Geo Coordinates (Latitude and Longitude), etc..\"], \"tutorial_steps\": [\"Let\\\\u2019s assume we have a table from an Excel file where one column represents the Country, and the other two columns represent the Actual Sales and Budget Sales as shown below: After we have logged in, we go to our Birst Space (or create a new Professional space) and then import the Excel file into Birst in the \\\\u2018Connect\\\\u2019 phase.\", \"Instructions for uploading the Excel file are included in this video on the official Infor channel.\", \"Simply put, the steps are: Connect to the Excel file & upload it.\", \"Ensure the data is correct.\", \"Import the data.\", \"Then, we move to the\\\\u00a0Prepare\\\\u00a0phase.\", \"This is where we clean up the data, or add calculations and scripts.\", \"For the sample data there is no need to do any Prepare actions.\", \"In\\\\u00a0Relate, we create connections, or joins, between data sources, if necessary.\", \"You can see how Relate works in the following video: The next step is to publish the data from the Staging Tables into the Data Model using the \\\\u2018Publish\\\\u2019 icon.\", \"Once the data is published, we go to the Visualizer and create a Geomap Report.\", \"We choose a measure, which in our case is \\\\u2018Sales_Actuals\\\\u2019 and a Geo attribute which in our case is \\\\u2018Country\\\\u2019.\", \"For the Geo Attribute, Birst prompts the user to select the most suitable option.\", \"We then configure the Conditional Formatting specifying the criteria and the color coding.\", \"In this example, we specify a condition where the location is colored green if Sales Amount is greater than the Budgeted Sales.\", \"Otherwise, it is red.\", \"The resulting visual highlights the regions accordingly enabling the user to quickly focus on the regions needing attention.\", \"The tutorial is for a Professional space, but you can of course do the same with data in an Enterprise space.\", \"Infor Birst How-To Series \\\\u2013 Connecting to Files Infor Birst How-To Series \\\\u2013 Prepare in a Professional Space Infor Birst How-To Series \\\\u2013 Relate in a Professional Space Creating Geomaps in Birst Help\"], \"feedback\": \"To main a professional tone, refrain from using words, like us or we. Provide more information for when to use a \\'Professional\\' space and when to use an \\'Enterprise\\' space. Also preface the tutorial by stating \\'The tutorial is for a Professional space\\' instead of at the end.\"}, {\"tutorial_title\": \"How to connect to a database\", \"business_problem\": \"Gathering data for data integration, analytics, and reporting can be time-consuming if your organization stores it in various databases across different systems and departments. Birst Cloud Agent allows for the centralization of your data sources, which removes the tedious task of accessing multiple systems separately. Additionally, Birst can help your organization access, obtain, and automatically pull the most up-to-date information for further analysis and reporting. Connecting databases through Birst is a safe process as sensitive data is protected and only accessible to authorized individuals. By using Birst, the analyzing and reporting process becomes easier for you, the user. You will be able to create reports, dashboards, and visualizations using data from different databases, all within a single platform.\", \"requirements\": [\"Access to Internet connection.\", \"Birst Analytics Platform account credentials.\", \"Birst Cloud Agent.\", \"Database \\\\n\\\\nConnectivity requirements depend on database vendor\\\\nObtain the desired database driver from the vendor and place it in the Birst Cloud Agent Drivers directory before connecting to it\\\\nInformation, such as database type, name, and login credentials.\", \"Connectivity requirements depend on database vendor.\", \"Obtain the desired database driver from the vendor and place it in the Birst Cloud Agent Drivers directory before connecting to it.\", \"Information, such as database type, name, and login credentials.\", \"Connectivity requirements depend on database vendor.\", \"Obtain the desired database driver from the vendor and place it in the Birst Cloud Agent Drivers directory before connecting to it.\", \"Information, such as database type, name, and login credentials.\"], \"tutorial_steps\": [\"Once you are successfully logged into your Birst account, create a new space, if none have been created already.\", \"To do so, click the \\\\u2018Create New Space\\\\u2019 hyperlink located in the middle of the Infor Birst welcome page.\", \"Otherwise, you can select an existing space.\", \"From the \\\\u2018Create New Space\\\\u2019 menu, choose to create an Enterprise, Professional, or Usage Tracking space.\", \"Make sure to name your space appropriately and save by clicking the green check button when finished.\", \"Enterprise: The most advanced and comprehensive edition, which offers support for complex data transformations, advanced scheduling, and robust governance and security controls.\", \"This is most suitable for large organizations with complex data integration requirements and a demand for a high level of governance and security.\", \"Professional: The mid-tier edition, which offers a balanced set of features for data integration but has limitations to its capabilities compared to the Enterprise edition.\", \"This is most suitable for medium-sized organizations with moderate data integration.\", \"Usage Tracking: A specialized edition that focuses on tracking and monitoring data usage and access patterns.\", \"This edition provides analytics and reporting capabilities to gain insights into how the data is being used within Birst.\", \"You are able to monitor user activity, track report usage, and analyze data consumption patterns.\", \"This is most suitable for organizations that want to understand user engagement and optimize their data analytic processes.\", \"To open an existing space, go to the \\\\u2018Global Navigation Menu\\\\u2019 and your current space is displayed at the top of the menu.\", \"Click on the \\\\u2018Switch Space\\\\u2019 button next to the name of your space and select your desired space.\", \"When the space is created, go to the \\\\u2018Modeler Connect\\\\u2019 page to create a connection to a data source.\", \"In the top left of the welcome page is the \\\\u2018Global Navigation Menu\\\\u2019 button, which is symbolized by 3 horizontal lines.\", \"Click the button to expand the menu, expand the \\\\u2018Modeler\\\\u2019 section, and the select \\\\u2018Connect\\\\u2019 tab.\", \"There are two ways to create a connection.\", \"In this example, the \\\\u2018SQL Databases\\\\u2018 option will be utilized.\", \"Click the add button to the right of connections, then select SQL Databases.\", \"In the \\\\u2018Create Connection\\\\u2019 side panel, provide the following information to create a connection to the SQL database:\\\\n\\\\nConnection Name: Decide on a meaningful and descriptive name that helps you identify this particular connection from others in your Birst environment.\", \"In this example, we will be naming this connection, SQL.\", \"Connection Type: Choose between \\\\u2018Import to Birst\\\\u2018 or \\\\u2018Live Access\\\\u2018.\", \"This tutorial will be sticking with the \\\\u2018Import to Birst\\\\u2018 option.\", \"Import to Birst: The data will be extracted, transformed, and loaded into the Birst platform.\", \"Data can be updated/refreshed periodically and incrementally, according to your requirements.\", \"This is the standard way of using data in Birst.\", \"Live Access: Live Access provides a real-time connection to data in local data marts and data warehouses without requiring that data to first be uploaded into the\\\\u00a0Birst\\\\u00a0data store (warehouse).\", \"With Live Access, a\\\\u00a0Birst\\\\u00a0logical model maps directly onto a local physical relational database.\", \"Suitable for real-time access.\", \"Agent Selection: Choose and assign a specific Birst Cloud Agent to perform tasks within the platform.\", \"Cloud databases: You can skip the agent by clicking on the toggle switch.\", \"Non-cloud databases: Assuming you already have an agent installed, you can select the \\\\u2018Select agent\\\\u2019 option and choose an active agent from the agent selection menu by clicking on the \\\\u2018Show online agents only\\\\u2019 button.\", \"If you do not already have an agent installed, refer to the How to Install Birst Cloud Agent in Infor Birst (youtube.com) video and tutorial.\", \"Database type: The specific category of your database system.\", \"Keep in mind that you will also need a driver, which serves as a communication bridge between Birst and the database.\", \"In this example, MSSQL will be used as the MS SQL Server JDBC driver comes with the Birst platform.\", \"However, if you are using a different type of database, you will need to download the driver yourself.\", \"MSSQL (Microsoft SQL Server): A relational database developed by Microsoft and has features for data storage, management, and retrieval.\", \"MySQL: A widely used open-source relational database that is used for web applications and is known for its speed, reliability, and ease of use.\", \"Oracle 11g: A relational database developed by Oracle and has features for data management, high availability, scalability, and security.\", \"This is most widely used in enterprise-level applications.\", \"PostgreSQL: An open-source object-relational database and has features for supporting JSON data, spatial data, and full-text search.\", \"Redshift: A cloud-based data warehouse developed by Amazon Web Services (AWS) and is optimized for quick querying and analyzing large datasets.\", \"SAP Hana: An in-memory relational database developed by SAP and is designed to efficiently store and process large quantities of data in real-time.\", \"Snowflake: A cloud-based data warehousing platform known for its scalability, performance, and easy to use.\", \"This has features that support structured and semi-structured data, data integration, analytics, and sharing data across organizations.\", \"Server Name: The hostname or the IP address of the server, where the database that you are connecting to is located.\", \"Database Name: An identifier assigned to the database.\", \"This helps Birst identify and connect to the correct database.\", \"Security Credentials: The username and password that is associated to an authorized user, who has permissions to access the database.\", \"*Optional: Below the \\\\u2018Security Credentials\\\\u2018 login box is a \\\\u2018More Options\\\\u2018 button.\", \"You can click this button if you want to specify fetch size, port, source prefix, and transaction isolation level.\", \"Fetch size: The number of rows that are retrieved from the database during a query.\", \"Port: A numeric value that represents the network port where the database server is communicating with the Birst platform.\", \"Source prefix: A string of text or an identifier that is added to the beginning of the names of imported data tables within Birst.\", \"If you want to add a source prefix, be sure to click on the slide to allow for providing the desired text.\", \"In this example, the prefix, \\\\u2018sql\\\\u2018, will be used so that the database being used is identified and duplicate tables from different sources are distinguished.\", \"Transaction isolation level: Determines the visibility of changes made by a single transaction to other concurrent transactions.\", \"Read committed: Transactions can only see changes committed by other transactions and provides better data consistency.\", \"Read uncommitted: Transactions can see changes made by other transactions, even if they are not committed yet and allows for high concurrency.\", \"But you are at risk of receiving inconsistent and potentially, incorrect data.\", \"Repeatable read: Regardless of any changes made, the data seen within a transaction remains the same.\", \"Serializable: Transactions are isolated from each other, and each transaction can only see the database as if it were the only one accessing it.\", \"When finished with providing the necessary information, click the green \\\\u2018Save\\\\u2018 button at the bottom of the menu.\", \"After the connection has been created, choose a schema.\", \"From the \\\\u2018schemas in SQL\\\\u2018 menu, check the boxes next to the desired schema name that you are going to work with.\", \"This tutorial will be selecting \\\\u2018dbo\\\\u2018.\", \"Click the blue \\\\u2018Apply\\\\u2018 at the bottom of the menu when finished.\", \"Similar to the previous step, import specific tables from the selected schema.\", \"Again, check the boxes next to the desired table names that you want to import.\", \"When you check the box, you will be allowed to preview the data.\", \"In this case, the \\\\u2018Accounts\\\\u2018 table will be imported.\", \"When you are done, click the blue \\\\u2018Done\\\\u2018 at the bottom of the menu.\", \"**Note: You can choose to not import certain columns by unchecking the box next to the column name.\", \"By default, all the columns will be imported.\", \"The \\\\u2018NetSuiteCustomerID\\\\u2018 column is unselected for this tutorial.\", \"**Note: You can also filter through the lines of each column by providing an expression indicating what rows are to be included or what rows are to be excluded.\", \"In this case, the \\\\u2018Segment\\\\u2018 column is filtered by selecting the \\\\u2018\\\\u2260 Not Equals\\\\u2018 operator and providing the value, \\\\u2018SMB\\\\u2018.\", \"Thus, the column will only contain the rows that are not labeled \\\\u2018SMB\\\\u2018.\", \"Another way to import data is using a query-based object.\", \"In the SQL import data menu, click the \\\\u2018\\\\u2026\\\\u2019 button to display the drop-down list for more options.\", \"Select the \\\\u2018Add query-based object\\\\u2018 option and provide the following information.\", \"When finished with providing the necessary information, click the green \\\\u2018Save\\\\u2018 button at the bottom of the menu.\", \"Source Name: The identifier assigned to the data table that you are retrieving data with a query.\", \"The name \\\\u2018SMB Accounts\\\\u2018 is used to represent the lines of data retrieved that contain \\\\u2018SMB\\\\u2018.\", \"Query: The request or command that is sent to the database to retrieve, manipulate, or modify the stored data.\", \"In this example, a query is written to bring in all the columns from the \\\\u2018Accounts\\\\u2018 table and the rows of the \\\\u2018Segment\\\\u2018 column contain \\\\u2018SMB\\\\u2018.\", \"SELECT * FROM Accounts WHERE Segment = \\\\u2018SMB\\\\u2019\\\\n\\\\n\\\\n\\\\n Preview the data tables that you want to import by selecting it from the list on the left.\", \"Click the green \\\\u2018Import SQL Data\\\\u2018 button when ready.\", \"Once the import is complete, go to the \\\\u2018Prepare\\\\u2019 page to view them.\", \"In the Modeler toolbar, select \\\\u2018Connect\\\\u2019, which will display a drop-down menu.\", \"From there, select the \\\\u2018Prepare\\\\u2018 option.\", \"In the \\\\u2018Prepare\\\\u2019 page, you can see that the imported data sources are grayed out, which means that they are in an \\\\u2018ignored state\\\\u2018.\", \"The \\\\u2018ignored state\\\\u2018 means that the data sources are not currently being used.\", \"However, you can still preview each data source by clicking on them from the \\\\u2018Sources\\\\u2019 menu.\"], \"feedback\": \"The steps are easy to follow and understand. You must capitalize the word\\'s first letter directly after a colon. The tone needs to remain formal. Refrain from using words, like we, ours, our, us, and let\\'s.\"}, {\"tutorial_title\": \"How to connect to a file\", \"business_problem\": \"Your organization may have many files and various file formats across different systems, that aren\\\\u2019t already in a database type storage. You wish to access these files using cloud analytics but don\\\\u2019t know how. Birst Cloud Agent allows for data consolidation and centralization by providing a unified view of data. Additionally, utilizing Birst allows you to access and integrate the most up-to-date information in real-time. Your job is simplified as Birst can automatically gather and transfer the required data for analysis reports. With Birst, only users with authorization can access the sensitive data, so your information is always protected.\", \"requirements\": [\"Birst Analytics Platform account permissions for:\\\\n\\\\nCreate Enterprise Spaces\\\\nCreate Professional Spaces.\", \"Create Enterprise Spaces.\", \"Create Professional Spaces.\", \"Active Birst Cloud Agent \\\\n\\\\nRefer to this tutorial for installing an agent: How to Install Birst Cloud Agent in Infor Birst\\\\nNeeded only if files are larger than 20MB.\", \"Refer to this tutorial for installing an agent: How to Install Birst Cloud Agent in Infor Birst.\", \"Needed only if files are larger than 20MB.\", \"Create Enterprise Spaces.\", \"Create Professional Spaces.\", \"Refer to this tutorial for installing an agent: How to Install Birst Cloud Agent in Infor Birst.\", \"Needed only if files are larger than 20MB.\"], \"tutorial_steps\": [\"Video example: Once you are successfully logged into your Birst account, create a new space, if none have been created already.\", \"To do so, click the \\\\u2018Create New Space\\\\u2019 hyperlink located in the middle of the Infor Birst welcome page.\", \"Otherwise, you can select an existing space.\", \"To create a new space, go to the \\\\u2018Create New Space\\\\u2019 menu and choose to create an Enterprise, Professional, or Usage Tracking space.\", \"Make sure to name your space appropriately and save by clicking the green check button when finished.\", \"In this tutorial, a Professional space will be utilized for the Infor Birst Essentials Guide and the associated data files is most compatible with this space.\", \"Of course, using files as a data source is compatible with the other spaces as well.\", \"Enterprise: The most advanced and comprehensive edition, which offers support for complex data transformations, advanced scheduling, and robust governance and security controls.\", \"This is most suitable for large organizations with complex data integration requirements and a demand for a high level of governance and security.\", \"Professional: The mid-tier edition, which offers a balanced set of features for data integration but has limitations to its capabilities compared to the Enterprise edition.\", \"This is most suitable for medium-sized organizations with moderate data integration.\", \"Usage Tracking: A specialized edition that focuses on tracking and monitoring data usage and access patterns.\", \"This edition provides analytics and reporting capabilities to gain insights into how the data is being used within Birst.\", \"You are able to monitor user activity, track report usage, and analyze data consumption patterns.\", \"This is most suitable for organizations that want to understand user engagement and optimize their data analytic processes.\", \"To open an existing space, go to the \\\\u2018Global Navigation Menu\\\\u2019 and your current space is displayed at the top of the menu.\", \"Click on the \\\\u2018Switch Space\\\\u2019 button next to the name of your space and select your desired space.\", \"When your space is created, go to the \\\\u2018Modeler Connect\\\\u2019 page to create a connection to a data source.\", \"In the top left of the welcome page is the \\\\u2018Global Navigation Menu\\\\u2019 button, which is symbolized by 3 horizontal lines.\", \"Click the button to expand the menu, expand the \\\\u2018Modeler\\\\u2019 section, and the select \\\\u2018Connect\\\\u2019 tab.\", \"There are two ways to connect to a file:\\\\n\\\\nWith the Quick Upload (does not need an Agent and can only upload up to 20MB)\\\\nUsing the Agent\\\\n\\\\nIt\\\\u2019s the\\\\u00a0Files\\\\u00a0option on the Modeler Connect page\\\\nFiles to be uploaded need to be in the Files folder\\\\nFiles can be bigger than 20MB\\\\nNeeds an agent\\\\n\\\\n\\\\n\\\\n At the top of the \\\\u2018Modeler Connect\\\\u2019 page is the \\\\u2018Quick upload\\\\u2018 feature that allows you to upload your files to Birst.\", \"Click on the \\\\u2018Browse Files\\\\u2019 button and select the files you want to upload from your computer.\", \"In this tutorial, the \\\\u2018Movie Titles.xlsx\\\\u2018 file will be uploaded.\", \"This feature supports the following file extensions: \\\\u2018.xls\\\\u2018, \\\\u2018.xlsx\\\\u2018, \\\\u2018.csv\\\\u2018, \\\\u2018.txt\\\\u2018, and \\\\u2018.zip\\\\u2018.\", \"The maximum file size that can be uploaded is 20 MB.\", \"Once the files have been successfully uploaded, you can preview the contents of the file.\", \"In the preview window, you can choose to not import certain columns of the file\\\\u2019s data tables by unchecking the box next to the column name.\", \"When the column is unselected, the entire column will be unhighlighted.\", \"Otherwise, the entire data table is highlighted in blue by default.\", \"At the bottom of the window are shortcuts (only available for files and not databases) to the different steps in the Birst data preparation process that you can skip to.\", \"The available shortcuts include: \\\\u2018Connect\\\\u2018, \\\\u2018Prepare\\\\u2018, \\\\u2018Relate\\\\u2018, \\\\u2018Visualizer\\\\u2018, and \\\\u2018Dashboards\\\\u2018.\", \"The \\\\u2018Prepare\\\\u2018 shortcut will be selected for this tutorial, but feel free to explore the other shortcuts.\", \"Connect: Establish a connection to your data sources within the Birst platform and configure the appropriate parameters to connect Birst with the correct data sources.\", \"Prepare: Transform the extracted data for analysis and report by cleaning, calculating, integrating, validating, modeling, enhancing with additional information, and safeguarding the data.\", \"Relate: Establish relationships between the different data sources by identifying and mapping key fields, defining relationships and hierarchies, and verifying the relationships to ensure accuracy.\", \"Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file\\\\u00a0and have Birst process it, and then take you directly to Visualizer to continue working on visualizations and reports.\", \"Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file\\\\u00a0and have Birst process it, and then take you directly to Dashboards to continue your work.\", \"After being taken to the \\\\u2018Modeler Prepare\\\\u2019 page, imported files are shown at the top of the \\\\u2018Sources\\\\u2019 menu.\", \"Additionally, make sure the data is in the correct place, so that the agent can access them.\", \"In order to do so, move your file(s) into the \\\\u2018files\\\\u2018 folder inside the Birst Cloud Agent installation folder.\", \"Another way to upload files is using an agent.\", \"For files over 20MB, you need to have an active Birst Cloud Agent available to handle and process the data.\", \"If you do not have an agent installed, refer to this video: How to Install Birst Cloud Agent in Infor Birst and tutorial.\", \"Go back to the Birst portal and go to the \\\\u2018Modeler Connect\\\\u2019 page to upload more files.\", \"Access the \\\\u2018Modeler Connect\\\\u2019 page by going to the \\\\u2018Modeler\\\\u2019 toolbar and selecting the \\\\u2018Prepare\\\\u2019 tab, which displays a drop-down menu.\", \"From that menu, select the \\\\u2018Connect\\\\u2019 option.\", \"In the center of the \\\\u2018Modeler Connect\\\\u2019 page, select the \\\\u2018Files\\\\u2018 option.\", \"Fill in the following information and click the green \\\\u2018Save\\\\u2019 button when finished:\\\\n\\\\nConnection Name: Decide on a meaningful and descriptive name that helps you identify this particular connection from others in your Birst environment.\", \"In this case, name this connection, \\\\u2018My Files\\\\u2018.\", \"Assign Agent: Choose and assign a specific Birst Cloud Agent to perform tasks within the platform.\", \"Click the \\\\u2018Select agent\\\\u2019 button and choose an active agent from the agent selection menu by clicking on the \\\\u2018Show online agents only\\\\u2019 button.\", \"Select the desired agent by checking the box next to that agent\\\\u2019s name.\", \"When finished, select the \\\\u2018Back\\\\u2019 option.\", \"File path: This field serves as a reminder for you to place any files that you want to import in the \\\\u2018files\\\\u2018 folder of the Birst Cloud Agent installation folder.\", \"From the \\\\u2018Select files for _(Connection Name)_\\\\u2019 menu, a list of imported files is displayed and you can select the files that you want uploaded by checking the box next to the file\\\\u2019s name.\", \"Click on the blue \\\\u2018Upload\\\\u2019 button when finished.\", \"When the selected files have been successfully uploaded, we can preview the data.\", \"You can choose to not import certain columns of the file\\\\u2019s data tables by unchecking the box next to the column name.\", \"When the column is unselected, the entire column will be unhighlighted.\", \"Otherwise, the entire data table is highlighted in blue by default.\", \"Click on the blue \\\\u2018Done\\\\u2019 button when finished.\", \"As you import and upload these files, you are actually copying the data from the data source, so that it can be stored within the Birst platform.\", \"This allows for faster and more efficient data access and analysis.\", \"Go to the \\\\u2018Modeler Prepare\\\\u2019 page by going to the \\\\u2018Modeler\\\\u2019 toolbar, selecting the \\\\u2018Connect\\\\u2019 tab, and then select the \\\\u2018Prepare\\\\u2019 option.\", \"Now, you can see all of the files that were imported and uploaded to the Birst platform.\"], \"feedback\": \"The steps are easy to follow and understand. Fix the wording for the Visualizer description because \\'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it, and then take you directly to Visualizer to continue working on visualizations and reports.\\' sounds odd. The fixed sentence should be \\'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it. It then takes you directly to Visualizer to continue working on visualizations and reports.\\'. Fix the wording for the Dashboards description because \\'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it, and then take you directly to Dashboards to continue your work.\\' sounds odd. The fixed sentence should be \\'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it. It then takes you directly to Dashboards to continue your work.\\'.\"}, {\"tutorial_title\": \"Keep Analytics Data Updated\", \"business_problem\": \"Users wants to see near real-time or latest data in analytic Reports and Dashboards. You want to avoid costly manual data imports and instead automate the process of keeping reports and dashboards up to date.\", \"requirements\": [], \"tutorial_steps\": [\"Analytics Birst Admin access (for Orchestration).\", \"If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation.\", \"For example, in M3 Analytics you need the \\\\u2018M3A Administration\\\\u2018 role for Admin access.\", \"Expected Completion Time: 15 minutes Keeping data fresh can be achieved by scheduling periodic updates.\", \"Let\\\\u2019s assume that we are extracting data tables from the Infor data lake to be used in Birst reports and dashboards.\", \"We have completed creating the dashboards and we want to update the data in Birst, automatically on a scheduled basis.\", \"For this purpose, we make use of the \\\\u2018Orchestration Workflow\\\\u2019, a simple process with a few clicks.\", \"We make use of \\\\u2018Extract Groups\\\\u2019 and \\\\u2018Publishing Groups\\\\u2019 to process batches of data Extract Groups can be created in order to group the tables that are part of the Orchestration Workflow.\", \"Hence, if we want to include only 5 tables in our process, out of the 20 tables included in our connection to the data lake, we use Extract Groups.\", \"Extract Groups are created in \\\\u2018Modeler Connect\\\\u2019 as shown below: we create an \\\\u2018Extraction Group\\\\u2019, name it, select the Data \\\\u2018Connections\\\\u2019 and then the individual tables within them.\", \"A similar concept is used for Publishing Groups.\", \"It helps us to select only those tables which we want to publish as part of \\\\u2018Orchestration Workflow\\\\u2019.\", \"Publishing Groups are created in \\\\u2018Modeler Prepare\\\\u2019 as shown below: we create a \\\\u2018Publishing Group\\\\u2019, name it and then select the Staging Tables to be put in.\", \"We create an \\\\u2018Orchestration Workflow\\\\u2019, which can be run manually or automatically.\", \"For our business problem, we will run the workflow automatically by scheduling it.\", \"While configuring \\\\u2018Orchestration Workflow\\\\u2019, we imagine the steps that we usually do when updating the data.\", \"First we create a Workflow and name it.\", \"We add steps for extracting the data from the data lake using \\\\u2018Extract Groups\\\\u2019, and then publishing the data using \\\\u2018Publishing Groups\\\\u2019 or all the data tables.\", \"Finally, we schedule the workflow as per requirements.\", \"An important factor to consider is to allow sufficient time window for the workflow to complete before the user checks the new data.\", \"The example below shows that the workflow will run on 2nd\\\\u00a0day of every month at 12:00 AM EST.\", \"Several schedules can be added as per requirements.\", \"Extracting Groups\"], \"feedback\": \"To main a professional tone, refrain from using words, like us or we. New terms are introduced and defined nicely.\"}, {\"tutorial_title\": \"Limit number of years/quarters/months visible in a filter\", \"business_problem\": \"Users are complaining that it is too slow and difficult to use the time filters because they can see everything from the year 1900 onwards. In Birst, there are 2 ways of limiting the number of time categories available in a filter.  Analytics\", \"requirements\": [\"Birst Admin access (for Space Properties).\", \"Edit Dashboard right (for dashboard filter editing).\", \"If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the \\\\u2018M3A Administration\\\\u2018 role for Admin and Visualizer access..\"], \"tutorial_steps\": [\"As a Space Admin, you can set a minimum year and a maximum year for filters that use the built-in Time attributes and time series measures in the\\\\u00a0space.\", \"The default values are  -1 and -1, translating into a default span of 150 years from 1900 to 2050.\", \"Best Practice: Change these values to match the time frame of your reporting requirements for the space.\", \"Restricting the time frame makes your reports more efficient and easier to develop.\", \"For example, if the source data for the space goes back only to the year 2000, you could set the minimum year to 2000, and you won\\\\u2019t see previous years in the Time filters on a report.\", \"Go to Space Management, then Space Properties and the Advanced Tab.\", \"You will find the Min/Max Years about halfway down the page.\", \"Change the Settings.\", \"They will be automatically saved.\", \"If you have the Edit Dashboard right, you can create filters for the dashboard.\", \"For Time categories, you can use the following types of filters: Checkbox.\", \"Radio button.\", \"Slider.\", \"Calendar.\", \"Checkbox and radio button filters are populated either with a query or a custom list.\", \"You can edit the query to include only the time categories required for the dashboard.\", \"This can also be made to work dynamically using variables like {Current Year} defined in Space Management/Manage Variables.\", \"See the online Help for Managing Variables.\", \"An example on how to edit the Filter query can be found here: If you want to include categories like Current Year and Prior Year in the filter then the Custom list is an awesome option.\", \"You have to list all the categories you want to have in the filter, and the values in the Value column have to be found in the data in your space/model.\", \"The Slider filter lets you choose the upper and lower limits in Custom Range.\", \"You can also limit the categories available with the Query.\", \"The Calendar filter type lets you set default values including start and end values using Variables.\", \"Dashboard Filters Min/Max Years in Space Properties Basic Dashboard Filtering in the Birst How-To Series on YouTube Advanced Dashboard Filtering in the Birst How-To Series on YouTube\"], \"feedback\": \"The business problem needs to be defined better because it is too vague. Best practice tips are helpful and make the tutorial easier to understand. When a feature has different options, explain what the options are so that the user knows which to pick.\"}]}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL to Tutorials page and Components page\n",
    "url_lst = ['https://developer.infor.com/components/analytics/', 'https://developer.infor.com/tutorials/analytics/']\n",
    "\n",
    "# Get all the URLs relating to the application \n",
    "# BIRST example: 9 tutorials and 1 components\n",
    "master_lst = get_urls(url_lst)\n",
    "\n",
    "# Build the json string \n",
    "master_dataset = json.loads(get_dataset(master_lst))\n",
    "\n",
    "for tutorial in master_dataset['tutorials']:\n",
    "    if tutorial['tutorial_title'] == 'Birst Cloud Agent Installation':\n",
    "        tutorial['feedback'] = \"The steps are easy to follow and understand. You must capitalize the word's first letter directly after a colon. The tone needs to remain formal. Refrain from using words, like we, ours, our, us, and let's.\"\n",
    "\n",
    "    elif tutorial['tutorial_title'] == 'How to connect to a database':\n",
    "        tutorial['feedback'] = \"The steps are easy to follow and understand. You must capitalize the word's first letter directly after a colon. The tone needs to remain formal. Refrain from using words, like we, ours, our, us, and let's.\"\n",
    "\n",
    "    elif tutorial['tutorial_title'] == 'How to connect to a file':\n",
    "        tutorial['feedback'] = \"The steps are easy to follow and understand. Fix the wording for the Visualizer description because 'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it, and then take you directly to Visualizer to continue working on visualizations and reports.' sounds odd. The fixed sentence should be 'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it. It then takes you directly to Visualizer to continue working on visualizations and reports.'. Fix the wording for the Dashboards description because 'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it, and then take you directly to Dashboards to continue your work.' sounds odd. The fixed sentence should be 'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it. It then takes you directly to Dashboards to continue your work.'.\" \n",
    "\n",
    "    elif tutorial['tutorial_title'] == 'Changing visualizations based on user selection':\n",
    "        tutorial['feedback'] = \"To main a professional tone, refrain from using words, like us or we. Otherwise, steps are easy to follow and understand.\"\n",
    "\n",
    "    elif tutorial['tutorial_title'] == 'Dashboard filters of dynamically changing Measures and Dimensions':\n",
    "        tutorial['feedback'] = \"To improve clarity, explain what the variables do, such as the 'GetPromptValue' variable has a description to explain its function. Provide more information about the difference between a 'measure' and a 'dimension' and when to use them.\"\n",
    "\n",
    "    elif tutorial['tutorial_title'] == 'Highlight the Performance on a Geographical Map':\n",
    "        tutorial['feedback'] = \"To main a professional tone, refrain from using words, like us or we. Provide more information for when to use a 'Professional' space and when to use an 'Enterprise' space. Also preface the tutorial by stating 'The tutorial is for a Professional space' instead of at the end.\"\n",
    "        \n",
    "    elif tutorial['tutorial_title'] == 'Dynamically change Time Series Types in Reports':\n",
    "        tutorial['feedback'] = \"Define 'trailing half year, trailing four months' for better understanding. Define 'Running Sum' and what it is used for. To main a professional tone, refrain from using words, like us or we.\"\n",
    "\n",
    "    elif tutorial['tutorial_title'] == 'Keep Analytics Data Updated':\n",
    "        tutorial['feedback'] = \"To main a professional tone, refrain from using words, like us or we. New terms are introduced and defined nicely.\"\n",
    "        \n",
    "    elif tutorial['tutorial_title'] == 'Limit number of years/quarters/months visible in a filter':\n",
    "        tutorial['feedback'] = \"The business problem needs to be defined better because it is too vague. Best practice tips are helpful and make the tutorial easier to understand. When a feature has different options, explain what the options are so that the user knows which to pick.\"\n",
    "        \n",
    "json.dumps(master_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53237e60-39f0-4fde-a1f3-567762785a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tutorial_title': 'Birst Cloud Agent Installation',\n",
       "  'business_problem': 'You wish to use cloud analytics for insights on your on-premise data but dont know how to connect them. Birst Cloud Agent (Birst) simplifies the process of transferring, accessing, and integrating data into a cloud-based analytics platform. Not only is the process more secure, reliable, and efficient, but Birst also eliminates the need for manual data extraction and manipulation. Birst serves as the bridge between on-premises data sources and the Birst cloud environment, which ensures that your data is up-to-date and accessible for accurate reporting and analysis in real-time. This way, you and your organization can make data-driven decisions quicker, improve operational efficiency, and gain valuable insights from your data across different systems and databases.',\n",
       "  'requirements': ['Internet Connection.',\n",
       "   'Birst Analytics Platform Account Credentials.',\n",
       "   'Birst Cloud Agent.',\n",
       "   'Data Sources.',\n",
       "   'Java 8 JDK or JRE.'],\n",
       "  'tutorial_steps': ['Log into Birst and create a new space.',\n",
       "   'To do so, click the Create New Space hyperlink located in the middle of the Infor Birst welcome page.',\n",
       "   'From the Create New Space menu, we can choose to create an Enterprise, Professional, or Usage Tracking space.',\n",
       "   'Make sure to name your space appropriately and save when finished.',\n",
       "   'Enterprise: the most advanced and comprehensive edition, which offers support for complex data transformations, advanced scheduling, and robust governance and security controls.',\n",
       "   'This is most suitable for large organizations with complex data integration requirements and a demand for a high level of governance and security.',\n",
       "   'Professional: the mid-tier edition, which offers a balanced set of features for data integration but has limitations to its capabilities compared to the Enterprise edition.',\n",
       "   'This is most suitable for medium-sized organizations with moderate data integration.',\n",
       "   'Usage Tracking: a specialized edition that focuses on tracking and monitoring data usage and access patterns.',\n",
       "   'This edition provides analytics and reporting capabilities to gain insights into how the data is being used within Birst.',\n",
       "   'You are able to monitor user activity, track report usage, and analyze data consumption patterns.',\n",
       "   'This is most suitable for organizations that want to understand user engagement and optimize their data analytic processes.',\n",
       "   'When the space is created, go to the Modeler Connect page to create a connection to a data source.',\n",
       "   'In the top left of the welcome page is the Global Navigation Menu button, which is symbolized by 3 horizontal lines.',\n",
       "   'Click the button to expand the menu, expand the Modeler section, and the select Connect tab.',\n",
       "   'Depending on your data source, you can select between the SQL Databases option or the Files option.',\n",
       "   'In this tutorial, we will be selecting the SQL Databases option.',\n",
       "   'SQL Databases: well-suited for large volumes of structured and relational data with predefined schemas.',\n",
       "   'Files: a more flexible option for CSV or Excel files and works best for a smaller volume of unstructured or semi-structured data.',\n",
       "   'Once you have made your selection, select and assign an agent.',\n",
       "   'Assigning an agent means you are telling Birst which agent is used to access a particular on-premise data source (or files).',\n",
       "   'Your agent needs to have access to the data source we are connecting to, and it is very probable an organization would have more than one agent running in a Birst environment.',\n",
       "   'If there are not any existing agents installed, simply select the Click here hyperlink (located below the grayed out Select agent button) to download the BirstCloudAgent.zip installation file.',\n",
       "   'Note: A Birst Agent is required when connecting to on-premise data sources.',\n",
       "   'After the zip file has downloaded, extract its contents and make note of their location.Optional: you can add an identifier to the end of the folder name to specify which environment is being connected to with the current agent.',\n",
       "   'In the sample video, we used the identifier _EU after extracting its contents to C:\\\\BirstCloudAgent_EU.',\n",
       "   'The agent can run in a batch mode or as a service.',\n",
       "   'In this tutorial, the agent will be running in batch mode.',\n",
       "   'If you want your agent to run as a service, refer to the instructions in the Readme.txt file.',\n",
       "   'Batch mode: the agent is executing tasks in a batch-like manner, either periodically or at scheduled intervals.',\n",
       "   'This is most suitable for scenarios where data changes occur at regular intervals.',\n",
       "   'Service: the agent remains active and ready to handle any data integration at any time, unlike batch mode.',\n",
       "   'The agent is operating as a continuous background service on a server or machine and constantly monitors for changes without waiting for a trigger or schedule.',\n",
       "   'This is most suitable for immediate or near-real-time data integration.',\n",
       "   'In the contents of the unzipped download  Locate the Agent.bat file in the bin folder.',\n",
       "   'There are three options to run the agent:\\n\\nYou can double click the bat-file\\nYou can right-click the bat-file and chose Run as Administrator (some security setups on computers require this to run bat-files).',\n",
       "   'Start the command prompt, navigate to the file, and run the file.',\n",
       "   'This is recommended for the first time use, so you can see what is going on with the agent.',\n",
       "   'After you see it run, you can use the above options.',\n",
       "   'Warning: common errors can occur when Java is not installed, the incorrect version is installed, or Java home has not been set.',\n",
       "   'Go back to the Infor Birst portal and click the Select Agent button.',\n",
       "   'In the agent selection menu, click the Refresh button because we have installed a new agent.',\n",
       "   'We only want to choose from the active agents only, so click the Show online agents only button, which is located directly below the Refresh button.',\n",
       "   'The newly installed agent should be located at the top of the list.',\n",
       "   'Select the check box next to the agent name to activate the agent.'],\n",
       "  'feedback': \"The steps are easy to follow and understand. You must capitalize the word's first letter directly after a colon. The tone needs to remain formal. Refrain from using words, like we, ours, our, us, and let's.\"},\n",
       " {'tutorial_title': 'Changing visualizations based on user selection',\n",
       "  'business_problem': 'You have a dashboard that displays the correct information, but in some situations the user would like to analyze the data from different perspectives using different visualizations. You would like to give them a selection option on the dashboard to choose the visualization they desire. Analytics',\n",
       "  'requirements': ['Edit Dashboard right (for dashboard filter editing).',\n",
       "   'If Disable Filter Edit Access permission is enabled, the user can only create and edit Local Filters..',\n",
       "   'If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the M3A Administration role for Edit Dashboard access..'],\n",
       "  'tutorial_steps': ['The\\xa0Birst View Selector\\xa0allows a user to switch between the original chart and a table view in a dashlet.',\n",
       "   'Dashboard creator enables or disables the View Selector for each dashlet.',\n",
       "   'Depending on how many measures and attributes your original chart uses, the View Selector will have a different number of chart options available.',\n",
       "   'However if you want to include a map view, or to switch between reports and charts that hold a different data set, you need a different approach.',\n",
       "   'By setting up a filter that controls which reports are visible you can give the user a way to easily switch between reports right on the dashboard.',\n",
       "   'You can use Data Driven Dashboards to overlay different dashlets on top of each other and show/hide them using a dashboard filter.',\n",
       "   'Showing and hiding dashlets (reports, KPIs, images) enables the user to change the way they analyze data while staying on a single dashboard.',\n",
       "   'Based on selections made on in the filters the reports can be shown or hidden, changing the design of the whole dashboard.',\n",
       "   'Create one or more mandatory filters as parameters that will deliver specific values to the condition.',\n",
       "   'In the video below we create a filter called Report Selector that will display three options: Map, Table, and Column.',\n",
       "   'Each option is mapped to its own value Map  Map, Table  Tab, Columns  Col.',\n",
       "   'These values are the output of the filter.',\n",
       "   'We change the filter from Multiple to Single selection so that it produces only one result, and we set it to Mandatory so that one of the options is always selected, even when the page loads.',\n",
       "   'Create conditions that use the values from the parameter filters.',\n",
       "   'Data Driven Dashboards allow us to show and hide any dashlet on the dashboard based on a condition.',\n",
       "   'In the video below we create a condition called Show Table, and we connect it to the filter we created earlier called Report Selector.',\n",
       "   'The condition uses the output from the filter, and when that output is equal to Tab then the condition is true.',\n",
       "   'Add reports, KPIs or any dashlet to the dashboard, and associate the conditions to the dashlet by first clicking the condition, and then the dashlet.',\n",
       "   'You will see a hammer icon on the bottom right corner of the dashlet associated with the condition.',\n",
       "   'As long as the condition is true the associated dashlet will be shown.',\n",
       "   'Once the parameter filter is changed the condition will become false and the dashlet will hide.',\n",
       "   'The user will make a selection on the embedded filter.',\n",
       "   'Based on the users selection the filter will generate the corresponding parameter.',\n",
       "   'And the condition will be either True or False.',\n",
       "   'If the condition is true, the associated dashlets\\xa0will be visible, otherwise they will remain hidden.',\n",
       "   'How to set up Dashboard Filters  How to setup Data Driven Dashboards Basic Dashboard Filtering in the Birst How-to series on YouTube Advanced Dashboard Filtering in the Birst How-to series on YouTube'],\n",
       "  'feedback': 'To main a professional tone, refrain from using words, like us or we. Otherwise, steps are easy to follow and understand.'},\n",
       " {'tutorial_title': 'Dashboard filters of dynamically changing Measures and Dimensions',\n",
       "  'business_problem': 'You have a dashboard that communicates valuable information, but you want the same report to switch between different metrics or dimensions. For example, a pie chart where an employee can toggle between revenue per city, to revenue by country, or revenue by sales manager.  Another example would be to switch between multiple measures on a single report, such as from Revenue per country, to number of customers per country.',\n",
       "  'requirements': ['Edit Dashboard\\xa0right (for dashboard filter editing).',\n",
       "   'If\\xa0Disable Filter Edit Access\\xa0permission is enabled, the user can only create and edit Local Filters..',\n",
       "   'If you are using\\xa0an Infor Cloudsuite, you\\xa0can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the M3A Administration role for Edit Dashboard access..'],\n",
       "  'tutorial_steps': ['The video below shows you how to create a filter with parameters that use the measures.',\n",
       "   'In Visualizer, Advanced Tools/BQL Editor, create a Saved Expression that will collect and interpret the values from the parameter.',\n",
       "   \"ex:\\xa0<EVAL!GetPromptValue(Value Selector,'[OrderDate: # Distinct CustomerID])!> GetPromptValue\\xa0 captures the value provided by the parameter called\\xa0Value Selector, as a string.\",\n",
       "   'If the parameter is empty, then a default string is used, in this case\\xa0[OrderDate: # Distinct CustomerID].',\n",
       "   '<EVAL!',\n",
       "   ' evaluates the string as a Measure or Dimension.',\n",
       "   '3.',\n",
       "   'Finally, use the newly created Saved Expression in your report as a regular Measure or Dimension.',\n",
       "   'The user will make a selection on the embedded filter.',\n",
       "   'Based on the users selection the filter will generate the corresponding parameter.',\n",
       "   'The\\xa0GetPromptValue\\xa0function will capture the value from the parameter, and\\xa0<EVAL!',\n",
       "   'will convert the captured string to a Measure or Dimension.',\n",
       "   'How to set up Dashboard Filters.',\n",
       "   'Basic Dashboard Filtering in the Birst How-to series on YouTube Advanced Dashboard Filtering in the Birst How-to series on YouTube Alternate solution:\\xa0Creating Column Selector'],\n",
       "  'feedback': \"To improve clarity, explain what the variables do, such as the 'GetPromptValue' variable has a description to explain its function. Provide more information about the difference between a 'measure' and a 'dimension' and when to use them.\"},\n",
       " {'tutorial_title': 'Dynamically change Time Series Types in Reports',\n",
       "  'business_problem': 'Users want to use different, non-standard time categories in their reports, and they prefer not having to use several reports or dashboard pages to do this. \\xa0 Analytics',\n",
       "  'requirements': ['The right to Edit Dashboards, use Visualizer, Saved Expressions..',\n",
       "   'A properly modeled dataset, specifically that the Measure comes from a fact table with a date, and that the date is connected to the\\xa0Time dimensions, in other words, that the Analyze By Date is toggled on..',\n",
       "   'If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the M3A Administration role for Admin and Visualizer access..'],\n",
       "  'tutorial_steps': ['Expected Completion Time: 10 minutes Infor Birst has a great ability to quickly compare data between different periods.',\n",
       "   'All thats needed is a measure, a date, and you can choose one of the following Time Series Types: When building reports, there is no way to create additional custom time categories, like Trailing Half Year, Trailing Four Months, or any Weekly Time Series.',\n",
       "   'Also there could be customers that need to analyze data weekly, and while access to extra time categories can be enabled through support there is a way to develop these and more time categories in Visualizer.',\n",
       "   'In situations like this we can use the power of the function RSUM and build a report that will show data based on a Time Series we define.',\n",
       "   'In addition to time categories RSUM can calculate the trailing sum across any dimension, including text such as for example roman numerals I, II, III, IV.',\n",
       "   'For the Trailing N Months, we can simply use Running Sum or RSUM.',\n",
       "   'Birst has a built in Trailing 3 Months which we can replicate with a function:\\xa0RSUM(window size,[ Measure]) where\\xa0window size\\xa0is the number of periods to consider, be it weeks, days or in this case, three months.',\n",
       "   'Using the BQL Editor, we can create expressions.',\n",
       "   'SavedExpression(Custom T3M) =\\xa0RSUM(3,[OrderDate: # Distinct OrderID]).',\n",
       "   'We can easily change this to be Trailing 4, 5, 6 or any number of periods by changing the window size.',\n",
       "   'You can see in the above image, that 01/2021 column only includes the data for that month.',\n",
       "   'This is controlled by the Filter/more options and can be changed.',\n",
       "   'When Use as a display filter is toggled off only the displayed data is used in the calculation.',\n",
       "   'When Use as a display filter is toggled on all data is used in the calculation, and then only the relevant data is displayed.',\n",
       "   'Below is an image of the same report with Use as a display filter toggled on.',\n",
       "   'Note that the same\\xa0RSUM(3,[Measure]) will show three months data if the Date Dimension is monthly, it will show three days data if the Date Dimension is daily, and it will show three years data if the Date Dimension is yearly.',\n",
       "   'Furthermore, RSUM has no sense of time.',\n",
       "   'So depending on the sorting used in the report, for January it can add Nov, Dec, and Jan when the months are sorted in descending order, but it will add Mar, Feb, Jan, if the order is ascending.',\n",
       "   'While Infor Birst already offers Month Ago, with RSUM we can achieve 2 months ago or\\xa0 5 months ago.',\n",
       "   'The measurement for Month Ago is RSUM(2,[Measure])  [Measure].',\n",
       "   'The measurement for 2 Months Ago is\\xa0 RSUM(3,[Measure])  RSUM(2,[Measure]).',\n",
       "   'The measurement for Quarter Ago is RSUM(4,[Measure])  RSUM(3,[Measure]).',\n",
       "   'The measurement for Year Ago is RSUM(13,[Measure])  RSUM(12,[Measure]).'],\n",
       "  'feedback': \"Define 'trailing half year, trailing four months' for better understanding. Define 'Running Sum' and what it is used for. To main a professional tone, refrain from using words, like us or we.\"},\n",
       " {'tutorial_title': 'Highlight the Performance on a Geographical Map',\n",
       "  'business_problem': 'You would like to create a dashboard of Sales Performance for different locations, highlighting the high-performing and low-performing locations on a map. A map visualization will occupy less space on dashboard while showing lot of information. Analytics To accomplish this task, we need to have:',\n",
       "  'requirements': ['The right to create a Professional space.',\n",
       "   'Birst Visualizer  access (for creating a report).',\n",
       "   'Edit Dashboard right (for adding the report to the dashboard).',\n",
       "   'If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the M3A Administration role for Visualizer and Edit Dashboard access.',\n",
       "   'Data containing a Geo Attribute, i.e. Country, City, State (for USA), Geo Coordinates (Latitude and Longitude), etc..'],\n",
       "  'tutorial_steps': ['Lets assume we have a table from an Excel file where one column represents the Country, and the other two columns represent the Actual Sales and Budget Sales as shown below: After we have logged in, we go to our Birst Space (or create a new Professional space) and then import the Excel file into Birst in the Connect phase.',\n",
       "   'Instructions for uploading the Excel file are included in this video on the official Infor channel.',\n",
       "   'Simply put, the steps are: Connect to the Excel file & upload it.',\n",
       "   'Ensure the data is correct.',\n",
       "   'Import the data.',\n",
       "   'Then, we move to the\\xa0Prepare\\xa0phase.',\n",
       "   'This is where we clean up the data, or add calculations and scripts.',\n",
       "   'For the sample data there is no need to do any Prepare actions.',\n",
       "   'In\\xa0Relate, we create connections, or joins, between data sources, if necessary.',\n",
       "   'You can see how Relate works in the following video: The next step is to publish the data from the Staging Tables into the Data Model using the Publish icon.',\n",
       "   'Once the data is published, we go to the Visualizer and create a Geomap Report.',\n",
       "   'We choose a measure, which in our case is Sales_Actuals and a Geo attribute which in our case is Country.',\n",
       "   'For the Geo Attribute, Birst prompts the user to select the most suitable option.',\n",
       "   'We then configure the Conditional Formatting specifying the criteria and the color coding.',\n",
       "   'In this example, we specify a condition where the location is colored green if Sales Amount is greater than the Budgeted Sales.',\n",
       "   'Otherwise, it is red.',\n",
       "   'The resulting visual highlights the regions accordingly enabling the user to quickly focus on the regions needing attention.',\n",
       "   'The tutorial is for a Professional space, but you can of course do the same with data in an Enterprise space.',\n",
       "   'Infor Birst How-To Series  Connecting to Files Infor Birst How-To Series  Prepare in a Professional Space Infor Birst How-To Series  Relate in a Professional Space Creating Geomaps in Birst Help'],\n",
       "  'feedback': \"To main a professional tone, refrain from using words, like us or we. Provide more information for when to use a 'Professional' space and when to use an 'Enterprise' space. Also preface the tutorial by stating 'The tutorial is for a Professional space' instead of at the end.\"},\n",
       " {'tutorial_title': 'How to connect to a database',\n",
       "  'business_problem': 'Gathering data for data integration, analytics, and reporting can be time-consuming if your organization stores it in various databases across different systems and departments. Birst Cloud Agent allows for the centralization of your data sources, which removes the tedious task of accessing multiple systems separately. Additionally, Birst can help your organization access, obtain, and automatically pull the most up-to-date information for further analysis and reporting. Connecting databases through Birst is a safe process as sensitive data is protected and only accessible to authorized individuals. By using Birst, the analyzing and reporting process becomes easier for you, the user. You will be able to create reports, dashboards, and visualizations using data from different databases, all within a single platform.',\n",
       "  'requirements': ['Access to Internet connection.',\n",
       "   'Birst Analytics Platform account credentials.',\n",
       "   'Birst Cloud Agent.',\n",
       "   'Database \\n\\nConnectivity requirements depend on database vendor\\nObtain the desired database driver from the vendor and place it in the Birst Cloud Agent Drivers directory before connecting to it\\nInformation, such as database type, name, and login credentials.',\n",
       "   'Connectivity requirements depend on database vendor.',\n",
       "   'Obtain the desired database driver from the vendor and place it in the Birst Cloud Agent Drivers directory before connecting to it.',\n",
       "   'Information, such as database type, name, and login credentials.',\n",
       "   'Connectivity requirements depend on database vendor.',\n",
       "   'Obtain the desired database driver from the vendor and place it in the Birst Cloud Agent Drivers directory before connecting to it.',\n",
       "   'Information, such as database type, name, and login credentials.'],\n",
       "  'tutorial_steps': ['Once you are successfully logged into your Birst account, create a new space, if none have been created already.',\n",
       "   'To do so, click the Create New Space hyperlink located in the middle of the Infor Birst welcome page.',\n",
       "   'Otherwise, you can select an existing space.',\n",
       "   'From the Create New Space menu, choose to create an Enterprise, Professional, or Usage Tracking space.',\n",
       "   'Make sure to name your space appropriately and save by clicking the green check button when finished.',\n",
       "   'Enterprise: The most advanced and comprehensive edition, which offers support for complex data transformations, advanced scheduling, and robust governance and security controls.',\n",
       "   'This is most suitable for large organizations with complex data integration requirements and a demand for a high level of governance and security.',\n",
       "   'Professional: The mid-tier edition, which offers a balanced set of features for data integration but has limitations to its capabilities compared to the Enterprise edition.',\n",
       "   'This is most suitable for medium-sized organizations with moderate data integration.',\n",
       "   'Usage Tracking: A specialized edition that focuses on tracking and monitoring data usage and access patterns.',\n",
       "   'This edition provides analytics and reporting capabilities to gain insights into how the data is being used within Birst.',\n",
       "   'You are able to monitor user activity, track report usage, and analyze data consumption patterns.',\n",
       "   'This is most suitable for organizations that want to understand user engagement and optimize their data analytic processes.',\n",
       "   'To open an existing space, go to the Global Navigation Menu and your current space is displayed at the top of the menu.',\n",
       "   'Click on the Switch Space button next to the name of your space and select your desired space.',\n",
       "   'When the space is created, go to the Modeler Connect page to create a connection to a data source.',\n",
       "   'In the top left of the welcome page is the Global Navigation Menu button, which is symbolized by 3 horizontal lines.',\n",
       "   'Click the button to expand the menu, expand the Modeler section, and the select Connect tab.',\n",
       "   'There are two ways to create a connection.',\n",
       "   'In this example, the SQL Databases option will be utilized.',\n",
       "   'Click the add button to the right of connections, then select SQL Databases.',\n",
       "   'In the Create Connection side panel, provide the following information to create a connection to the SQL database:\\n\\nConnection Name: Decide on a meaningful and descriptive name that helps you identify this particular connection from others in your Birst environment.',\n",
       "   'In this example, we will be naming this connection, SQL.',\n",
       "   'Connection Type: Choose between Import to Birst or Live Access.',\n",
       "   'This tutorial will be sticking with the Import to Birst option.',\n",
       "   'Import to Birst: The data will be extracted, transformed, and loaded into the Birst platform.',\n",
       "   'Data can be updated/refreshed periodically and incrementally, according to your requirements.',\n",
       "   'This is the standard way of using data in Birst.',\n",
       "   'Live Access: Live Access provides a real-time connection to data in local data marts and data warehouses without requiring that data to first be uploaded into the\\xa0Birst\\xa0data store (warehouse).',\n",
       "   'With Live Access, a\\xa0Birst\\xa0logical model maps directly onto a local physical relational database.',\n",
       "   'Suitable for real-time access.',\n",
       "   'Agent Selection: Choose and assign a specific Birst Cloud Agent to perform tasks within the platform.',\n",
       "   'Cloud databases: You can skip the agent by clicking on the toggle switch.',\n",
       "   'Non-cloud databases: Assuming you already have an agent installed, you can select the Select agent option and choose an active agent from the agent selection menu by clicking on the Show online agents only button.',\n",
       "   'If you do not already have an agent installed, refer to the How to Install Birst Cloud Agent in Infor Birst (youtube.com) video and tutorial.',\n",
       "   'Database type: The specific category of your database system.',\n",
       "   'Keep in mind that you will also need a driver, which serves as a communication bridge between Birst and the database.',\n",
       "   'In this example, MSSQL will be used as the MS SQL Server JDBC driver comes with the Birst platform.',\n",
       "   'However, if you are using a different type of database, you will need to download the driver yourself.',\n",
       "   'MSSQL (Microsoft SQL Server): A relational database developed by Microsoft and has features for data storage, management, and retrieval.',\n",
       "   'MySQL: A widely used open-source relational database that is used for web applications and is known for its speed, reliability, and ease of use.',\n",
       "   'Oracle 11g: A relational database developed by Oracle and has features for data management, high availability, scalability, and security.',\n",
       "   'This is most widely used in enterprise-level applications.',\n",
       "   'PostgreSQL: An open-source object-relational database and has features for supporting JSON data, spatial data, and full-text search.',\n",
       "   'Redshift: A cloud-based data warehouse developed by Amazon Web Services (AWS) and is optimized for quick querying and analyzing large datasets.',\n",
       "   'SAP Hana: An in-memory relational database developed by SAP and is designed to efficiently store and process large quantities of data in real-time.',\n",
       "   'Snowflake: A cloud-based data warehousing platform known for its scalability, performance, and easy to use.',\n",
       "   'This has features that support structured and semi-structured data, data integration, analytics, and sharing data across organizations.',\n",
       "   'Server Name: The hostname or the IP address of the server, where the database that you are connecting to is located.',\n",
       "   'Database Name: An identifier assigned to the database.',\n",
       "   'This helps Birst identify and connect to the correct database.',\n",
       "   'Security Credentials: The username and password that is associated to an authorized user, who has permissions to access the database.',\n",
       "   '*Optional: Below the Security Credentials login box is a More Options button.',\n",
       "   'You can click this button if you want to specify fetch size, port, source prefix, and transaction isolation level.',\n",
       "   'Fetch size: The number of rows that are retrieved from the database during a query.',\n",
       "   'Port: A numeric value that represents the network port where the database server is communicating with the Birst platform.',\n",
       "   'Source prefix: A string of text or an identifier that is added to the beginning of the names of imported data tables within Birst.',\n",
       "   'If you want to add a source prefix, be sure to click on the slide to allow for providing the desired text.',\n",
       "   'In this example, the prefix, sql, will be used so that the database being used is identified and duplicate tables from different sources are distinguished.',\n",
       "   'Transaction isolation level: Determines the visibility of changes made by a single transaction to other concurrent transactions.',\n",
       "   'Read committed: Transactions can only see changes committed by other transactions and provides better data consistency.',\n",
       "   'Read uncommitted: Transactions can see changes made by other transactions, even if they are not committed yet and allows for high concurrency.',\n",
       "   'But you are at risk of receiving inconsistent and potentially, incorrect data.',\n",
       "   'Repeatable read: Regardless of any changes made, the data seen within a transaction remains the same.',\n",
       "   'Serializable: Transactions are isolated from each other, and each transaction can only see the database as if it were the only one accessing it.',\n",
       "   'When finished with providing the necessary information, click the green Save button at the bottom of the menu.',\n",
       "   'After the connection has been created, choose a schema.',\n",
       "   'From the schemas in SQL menu, check the boxes next to the desired schema name that you are going to work with.',\n",
       "   'This tutorial will be selecting dbo.',\n",
       "   'Click the blue Apply at the bottom of the menu when finished.',\n",
       "   'Similar to the previous step, import specific tables from the selected schema.',\n",
       "   'Again, check the boxes next to the desired table names that you want to import.',\n",
       "   'When you check the box, you will be allowed to preview the data.',\n",
       "   'In this case, the Accounts table will be imported.',\n",
       "   'When you are done, click the blue Done at the bottom of the menu.',\n",
       "   '**Note: You can choose to not import certain columns by unchecking the box next to the column name.',\n",
       "   'By default, all the columns will be imported.',\n",
       "   'The NetSuiteCustomerID column is unselected for this tutorial.',\n",
       "   '**Note: You can also filter through the lines of each column by providing an expression indicating what rows are to be included or what rows are to be excluded.',\n",
       "   'In this case, the Segment column is filtered by selecting the  Not Equals operator and providing the value, SMB.',\n",
       "   'Thus, the column will only contain the rows that are not labeled SMB.',\n",
       "   'Another way to import data is using a query-based object.',\n",
       "   'In the SQL import data menu, click the  button to display the drop-down list for more options.',\n",
       "   'Select the Add query-based object option and provide the following information.',\n",
       "   'When finished with providing the necessary information, click the green Save button at the bottom of the menu.',\n",
       "   'Source Name: The identifier assigned to the data table that you are retrieving data with a query.',\n",
       "   'The name SMB Accounts is used to represent the lines of data retrieved that contain SMB.',\n",
       "   'Query: The request or command that is sent to the database to retrieve, manipulate, or modify the stored data.',\n",
       "   'In this example, a query is written to bring in all the columns from the Accounts table and the rows of the Segment column contain SMB.',\n",
       "   'SELECT * FROM Accounts WHERE Segment = SMB\\n\\n\\n\\n Preview the data tables that you want to import by selecting it from the list on the left.',\n",
       "   'Click the green Import SQL Data button when ready.',\n",
       "   'Once the import is complete, go to the Prepare page to view them.',\n",
       "   'In the Modeler toolbar, select Connect, which will display a drop-down menu.',\n",
       "   'From there, select the Prepare option.',\n",
       "   'In the Prepare page, you can see that the imported data sources are grayed out, which means that they are in an ignored state.',\n",
       "   'The ignored state means that the data sources are not currently being used.',\n",
       "   'However, you can still preview each data source by clicking on them from the Sources menu.'],\n",
       "  'feedback': \"The steps are easy to follow and understand. You must capitalize the word's first letter directly after a colon. The tone needs to remain formal. Refrain from using words, like we, ours, our, us, and let's.\"},\n",
       " {'tutorial_title': 'How to connect to a file',\n",
       "  'business_problem': 'Your organization may have many files and various file formats across different systems, that arent already in a database type storage. You wish to access these files using cloud analytics but dont know how. Birst Cloud Agent allows for data consolidation and centralization by providing a unified view of data. Additionally, utilizing Birst allows you to access and integrate the most up-to-date information in real-time. Your job is simplified as Birst can automatically gather and transfer the required data for analysis reports. With Birst, only users with authorization can access the sensitive data, so your information is always protected.',\n",
       "  'requirements': ['Birst Analytics Platform account permissions for:\\n\\nCreate Enterprise Spaces\\nCreate Professional Spaces.',\n",
       "   'Create Enterprise Spaces.',\n",
       "   'Create Professional Spaces.',\n",
       "   'Active Birst Cloud Agent \\n\\nRefer to this tutorial for installing an agent: How to Install Birst Cloud Agent in Infor Birst\\nNeeded only if files are larger than 20MB.',\n",
       "   'Refer to this tutorial for installing an agent: How to Install Birst Cloud Agent in Infor Birst.',\n",
       "   'Needed only if files are larger than 20MB.',\n",
       "   'Create Enterprise Spaces.',\n",
       "   'Create Professional Spaces.',\n",
       "   'Refer to this tutorial for installing an agent: How to Install Birst Cloud Agent in Infor Birst.',\n",
       "   'Needed only if files are larger than 20MB.'],\n",
       "  'tutorial_steps': ['Video example: Once you are successfully logged into your Birst account, create a new space, if none have been created already.',\n",
       "   'To do so, click the Create New Space hyperlink located in the middle of the Infor Birst welcome page.',\n",
       "   'Otherwise, you can select an existing space.',\n",
       "   'To create a new space, go to the Create New Space menu and choose to create an Enterprise, Professional, or Usage Tracking space.',\n",
       "   'Make sure to name your space appropriately and save by clicking the green check button when finished.',\n",
       "   'In this tutorial, a Professional space will be utilized for the Infor Birst Essentials Guide and the associated data files is most compatible with this space.',\n",
       "   'Of course, using files as a data source is compatible with the other spaces as well.',\n",
       "   'Enterprise: The most advanced and comprehensive edition, which offers support for complex data transformations, advanced scheduling, and robust governance and security controls.',\n",
       "   'This is most suitable for large organizations with complex data integration requirements and a demand for a high level of governance and security.',\n",
       "   'Professional: The mid-tier edition, which offers a balanced set of features for data integration but has limitations to its capabilities compared to the Enterprise edition.',\n",
       "   'This is most suitable for medium-sized organizations with moderate data integration.',\n",
       "   'Usage Tracking: A specialized edition that focuses on tracking and monitoring data usage and access patterns.',\n",
       "   'This edition provides analytics and reporting capabilities to gain insights into how the data is being used within Birst.',\n",
       "   'You are able to monitor user activity, track report usage, and analyze data consumption patterns.',\n",
       "   'This is most suitable for organizations that want to understand user engagement and optimize their data analytic processes.',\n",
       "   'To open an existing space, go to the Global Navigation Menu and your current space is displayed at the top of the menu.',\n",
       "   'Click on the Switch Space button next to the name of your space and select your desired space.',\n",
       "   'When your space is created, go to the Modeler Connect page to create a connection to a data source.',\n",
       "   'In the top left of the welcome page is the Global Navigation Menu button, which is symbolized by 3 horizontal lines.',\n",
       "   'Click the button to expand the menu, expand the Modeler section, and the select Connect tab.',\n",
       "   'There are two ways to connect to a file:\\n\\nWith the Quick Upload (does not need an Agent and can only upload up to 20MB)\\nUsing the Agent\\n\\nIts the\\xa0Files\\xa0option on the Modeler Connect page\\nFiles to be uploaded need to be in the Files folder\\nFiles can be bigger than 20MB\\nNeeds an agent\\n\\n\\n\\n At the top of the Modeler Connect page is the Quick upload feature that allows you to upload your files to Birst.',\n",
       "   'Click on the Browse Files button and select the files you want to upload from your computer.',\n",
       "   'In this tutorial, the Movie Titles.xlsx file will be uploaded.',\n",
       "   'This feature supports the following file extensions: .xls, .xlsx, .csv, .txt, and .zip.',\n",
       "   'The maximum file size that can be uploaded is 20 MB.',\n",
       "   'Once the files have been successfully uploaded, you can preview the contents of the file.',\n",
       "   'In the preview window, you can choose to not import certain columns of the files data tables by unchecking the box next to the column name.',\n",
       "   'When the column is unselected, the entire column will be unhighlighted.',\n",
       "   'Otherwise, the entire data table is highlighted in blue by default.',\n",
       "   'At the bottom of the window are shortcuts (only available for files and not databases) to the different steps in the Birst data preparation process that you can skip to.',\n",
       "   'The available shortcuts include: Connect, Prepare, Relate, Visualizer, and Dashboards.',\n",
       "   'The Prepare shortcut will be selected for this tutorial, but feel free to explore the other shortcuts.',\n",
       "   'Connect: Establish a connection to your data sources within the Birst platform and configure the appropriate parameters to connect Birst with the correct data sources.',\n",
       "   'Prepare: Transform the extracted data for analysis and report by cleaning, calculating, integrating, validating, modeling, enhancing with additional information, and safeguarding the data.',\n",
       "   'Relate: Establish relationships between the different data sources by identifying and mapping key fields, defining relationships and hierarchies, and verifying the relationships to ensure accuracy.',\n",
       "   'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file\\xa0and have Birst process it, and then take you directly to Visualizer to continue working on visualizations and reports.',\n",
       "   'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file\\xa0and have Birst process it, and then take you directly to Dashboards to continue your work.',\n",
       "   'After being taken to the Modeler Prepare page, imported files are shown at the top of the Sources menu.',\n",
       "   'Additionally, make sure the data is in the correct place, so that the agent can access them.',\n",
       "   'In order to do so, move your file(s) into the files folder inside the Birst Cloud Agent installation folder.',\n",
       "   'Another way to upload files is using an agent.',\n",
       "   'For files over 20MB, you need to have an active Birst Cloud Agent available to handle and process the data.',\n",
       "   'If you do not have an agent installed, refer to this video: How to Install Birst Cloud Agent in Infor Birst and tutorial.',\n",
       "   'Go back to the Birst portal and go to the Modeler Connect page to upload more files.',\n",
       "   'Access the Modeler Connect page by going to the Modeler toolbar and selecting the Prepare tab, which displays a drop-down menu.',\n",
       "   'From that menu, select the Connect option.',\n",
       "   'In the center of the Modeler Connect page, select the Files option.',\n",
       "   'Fill in the following information and click the green Save button when finished:\\n\\nConnection Name: Decide on a meaningful and descriptive name that helps you identify this particular connection from others in your Birst environment.',\n",
       "   'In this case, name this connection, My Files.',\n",
       "   'Assign Agent: Choose and assign a specific Birst Cloud Agent to perform tasks within the platform.',\n",
       "   'Click the Select agent button and choose an active agent from the agent selection menu by clicking on the Show online agents only button.',\n",
       "   'Select the desired agent by checking the box next to that agents name.',\n",
       "   'When finished, select the Back option.',\n",
       "   'File path: This field serves as a reminder for you to place any files that you want to import in the files folder of the Birst Cloud Agent installation folder.',\n",
       "   'From the Select files for _(Connection Name)_ menu, a list of imported files is displayed and you can select the files that you want uploaded by checking the box next to the files name.',\n",
       "   'Click on the blue Upload button when finished.',\n",
       "   'When the selected files have been successfully uploaded, we can preview the data.',\n",
       "   'You can choose to not import certain columns of the files data tables by unchecking the box next to the column name.',\n",
       "   'When the column is unselected, the entire column will be unhighlighted.',\n",
       "   'Otherwise, the entire data table is highlighted in blue by default.',\n",
       "   'Click on the blue Done button when finished.',\n",
       "   'As you import and upload these files, you are actually copying the data from the data source, so that it can be stored within the Birst platform.',\n",
       "   'This allows for faster and more efficient data access and analysis.',\n",
       "   'Go to the Modeler Prepare page by going to the Modeler toolbar, selecting the Connect tab, and then select the Prepare option.',\n",
       "   'Now, you can see all of the files that were imported and uploaded to the Birst platform.'],\n",
       "  'feedback': \"The steps are easy to follow and understand. Fix the wording for the Visualizer description because 'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it, and then take you directly to Visualizer to continue working on visualizations and reports.' sounds odd. The fixed sentence should be 'Visualizer: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it. It then takes you directly to Visualizer to continue working on visualizations and reports.'. Fix the wording for the Dashboards description because 'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it, and then take you directly to Dashboards to continue your work.' sounds odd. The fixed sentence should be 'Dashboards: Once you have created your model and processed the data once, you can update your data by uploading a new file and have Birst process it. It then takes you directly to Dashboards to continue your work.'.\"},\n",
       " {'tutorial_title': 'Keep Analytics Data Updated',\n",
       "  'business_problem': 'Users wants to see near real-time or latest data in analytic Reports and Dashboards. You want to avoid costly manual data imports and instead automate the process of keeping reports and dashboards up to date.',\n",
       "  'requirements': [],\n",
       "  'tutorial_steps': ['Analytics Birst Admin access (for Orchestration).',\n",
       "   'If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation.',\n",
       "   'For example, in M3 Analytics you need the M3A Administration role for Admin access.',\n",
       "   'Expected Completion Time: 15 minutes Keeping data fresh can be achieved by scheduling periodic updates.',\n",
       "   'Lets assume that we are extracting data tables from the Infor data lake to be used in Birst reports and dashboards.',\n",
       "   'We have completed creating the dashboards and we want to update the data in Birst, automatically on a scheduled basis.',\n",
       "   'For this purpose, we make use of the Orchestration Workflow, a simple process with a few clicks.',\n",
       "   'We make use of Extract Groups and Publishing Groups to process batches of data Extract Groups can be created in order to group the tables that are part of the Orchestration Workflow.',\n",
       "   'Hence, if we want to include only 5 tables in our process, out of the 20 tables included in our connection to the data lake, we use Extract Groups.',\n",
       "   'Extract Groups are created in Modeler Connect as shown below: we create an Extraction Group, name it, select the Data Connections and then the individual tables within them.',\n",
       "   'A similar concept is used for Publishing Groups.',\n",
       "   'It helps us to select only those tables which we want to publish as part of Orchestration Workflow.',\n",
       "   'Publishing Groups are created in Modeler Prepare as shown below: we create a Publishing Group, name it and then select the Staging Tables to be put in.',\n",
       "   'We create an Orchestration Workflow, which can be run manually or automatically.',\n",
       "   'For our business problem, we will run the workflow automatically by scheduling it.',\n",
       "   'While configuring Orchestration Workflow, we imagine the steps that we usually do when updating the data.',\n",
       "   'First we create a Workflow and name it.',\n",
       "   'We add steps for extracting the data from the data lake using Extract Groups, and then publishing the data using Publishing Groups or all the data tables.',\n",
       "   'Finally, we schedule the workflow as per requirements.',\n",
       "   'An important factor to consider is to allow sufficient time window for the workflow to complete before the user checks the new data.',\n",
       "   'The example below shows that the workflow will run on 2nd\\xa0day of every month at 12:00 AM EST.',\n",
       "   'Several schedules can be added as per requirements.',\n",
       "   'Extracting Groups'],\n",
       "  'feedback': 'To main a professional tone, refrain from using words, like us or we. New terms are introduced and defined nicely.'},\n",
       " {'tutorial_title': 'Limit number of years/quarters/months visible in a filter',\n",
       "  'business_problem': 'Users are complaining that it is too slow and difficult to use the time filters because they can see everything from the year 1900 onwards. In Birst, there are 2 ways of limiting the number of time categories available in a filter.  Analytics',\n",
       "  'requirements': ['Birst Admin access (for Space Properties).',\n",
       "   'Edit Dashboard right (for dashboard filter editing).',\n",
       "   'If you are using an Infor Cloudsuite, you can find the corresponding IFS roles in the CloudSuite Analytics documentation. For example, in M3 Analytics you need the M3A Administration role for Admin and Visualizer access..'],\n",
       "  'tutorial_steps': ['As a Space Admin, you can set a minimum year and a maximum year for filters that use the built-in Time attributes and time series measures in the\\xa0space.',\n",
       "   'The default values are  -1 and -1, translating into a default span of 150 years from 1900 to 2050.',\n",
       "   'Best Practice: Change these values to match the time frame of your reporting requirements for the space.',\n",
       "   'Restricting the time frame makes your reports more efficient and easier to develop.',\n",
       "   'For example, if the source data for the space goes back only to the year 2000, you could set the minimum year to 2000, and you wont see previous years in the Time filters on a report.',\n",
       "   'Go to Space Management, then Space Properties and the Advanced Tab.',\n",
       "   'You will find the Min/Max Years about halfway down the page.',\n",
       "   'Change the Settings.',\n",
       "   'They will be automatically saved.',\n",
       "   'If you have the Edit Dashboard right, you can create filters for the dashboard.',\n",
       "   'For Time categories, you can use the following types of filters: Checkbox.',\n",
       "   'Radio button.',\n",
       "   'Slider.',\n",
       "   'Calendar.',\n",
       "   'Checkbox and radio button filters are populated either with a query or a custom list.',\n",
       "   'You can edit the query to include only the time categories required for the dashboard.',\n",
       "   'This can also be made to work dynamically using variables like {Current Year} defined in Space Management/Manage Variables.',\n",
       "   'See the online Help for Managing Variables.',\n",
       "   'An example on how to edit the Filter query can be found here: If you want to include categories like Current Year and Prior Year in the filter then the Custom list is an awesome option.',\n",
       "   'You have to list all the categories you want to have in the filter, and the values in the Value column have to be found in the data in your space/model.',\n",
       "   'The Slider filter lets you choose the upper and lower limits in Custom Range.',\n",
       "   'You can also limit the categories available with the Query.',\n",
       "   'The Calendar filter type lets you set default values including start and end values using Variables.',\n",
       "   'Dashboard Filters Min/Max Years in Space Properties Basic Dashboard Filtering in the Birst How-To Series on YouTube Advanced Dashboard Filtering in the Birst How-To Series on YouTube'],\n",
       "  'feedback': 'The business problem needs to be defined better because it is too vague. Best practice tips are helpful and make the tutorial easier to understand. When a feature has different options, explain what the options are so that the user knows which to pick.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dataset['tutorials']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cba75-840d-4d3e-83af-a5041eee0bab",
   "metadata": {},
   "source": [
    "# Setting up and running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a197815-b726-45ce-8192-ac2d468fa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pre_data(examples):\n",
    "\n",
    "    # Combine title and description for input text\n",
    "    inputs = [\n",
    "        f\"{title} - {description}\" \n",
    "        for title, description in zip(examples[\"application_title\"], examples[\"application_description\"])\n",
    "    ]\n",
    "\n",
    "    # print(inputs)\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Create labels for unsupervised pre-training\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b5c601-6b2d-4df9-8a54-e8226352e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(examples):\n",
    "\n",
    "    # Extract individual lists from examples dictionary\n",
    "    business_problems = examples[\"business_problem\"]\n",
    "    tutorial_titles = examples[\"tutorial_title\"]\n",
    "    tutorial_steps = examples[\"tutorial_steps\"]\n",
    "    feedback = examples[\"feedback\"]\n",
    "\n",
    "    # Format inputs to include business problem, tutorial title, and steps\n",
    "    inputs = [\n",
    "        f\"Business Problem: {problem} Tutorial Title: {title} Tutorial Steps: {steps}\" \n",
    "        for problem, title, steps in zip(business_problems, tutorial_titles, tutorial_steps)\n",
    "    ]\n",
    "\n",
    "    # Format targets to just include the feedback \n",
    "    targets = feedback\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_targets = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs['labels'] = model_targets['input_ids']\n",
    "\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4beff798-1159-4ac4-8be7-8c4cd8a46201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376f79aaca8648f3824f9dd1941d55ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:03, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.977300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=3.7452583789825438, metrics={'train_runtime': 66.9911, 'train_samples_per_second': 0.373, 'train_steps_per_second': 0.373, 'total_flos': 3383545036800.0, 'train_loss': 3.7452583789825438, 'epoch': 25.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Prepare the pre-training dataset\n",
    "pretraining_data = [\n",
    "    {\n",
    "    'application_title': master_dataset['application_title'],\n",
    "    'application_description': master_dataset['application_description']\n",
    "    }\n",
    "]\n",
    "\n",
    "pre_dataset = Dataset.from_list(pretraining_data)\n",
    "tokenized_pre_dataset = pre_dataset.map(preprocess_pre_data, batched=True)\n",
    "\n",
    "# Run the model (pre-train)\n",
    "pre_training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=25,\n",
    "    logging_dir='./project_logs',\n",
    "    logging_steps=2\n",
    ")\n",
    "\n",
    "pre_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=pre_training_args,\n",
    "    train_dataset=tokenized_pre_dataset,\n",
    ")\n",
    "\n",
    "pre_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1e58985-031f-4ac5-976f-87fde8afdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \n",
    "    # Extract individual lists from examples dictionary\n",
    "    # business_problems = examples[\"business_problem\"]\n",
    "    # tutorial_titles = examples[\"tutorial_title\"]\n",
    "    tutorial_steps = examples[\"tutorial_steps\"]\n",
    "    targets = examples[\"feedback\"]\n",
    "\n",
    "    custom_prefix = \"Review the tutorial:\"\n",
    "\n",
    "    # Format inputs to include business problem, tutorial title, and steps\n",
    "    inputs = [\n",
    "        # f\"Business Problem: {problem} Tutorial Title: {title} Tutorial Steps: {steps}\"\n",
    "        f\"{custom_prefix} {steps}\"\n",
    "        for steps in tutorial_steps\n",
    "        # for problem, title, steps in zip(business_problems, tutorial_titles, tutorial_steps)\n",
    "    ]\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    labels = tokenizer(text_target=targets, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd5b3d66-b433-4c94-997e-8b1d64abc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset\n",
    "training_data = []\n",
    "\n",
    "# Preparing training dataset\n",
    "for entry in master_dataset['tutorials']:\n",
    "    business_problem = entry['business_problem']\n",
    "    title = entry['tutorial_title']\n",
    "    steps = entry['tutorial_steps']\n",
    "    feedback = entry['feedback']\n",
    "          \n",
    "    input = {\n",
    "        # 'business_problem': business_problem,\n",
    "        # 'tutorial_title': title,\n",
    "        'tutorial_steps': steps,\n",
    "        'feedback': feedback\n",
    "    }\n",
    "    \n",
    "    training_data.append(input)\n",
    "\n",
    "# Add more tutorials to dataset \n",
    "steps = \"Set up your Birst environment and create a space that is most appropriate for your project. Connect to Data sources by clicking on the 'add new source' option in the data sources section. We can connect to databases, cloud services, or other files. Extract and import the data from the connected source. Go to the data preparation section and perform data cleansing actions. Apply transformations to normalize the data and validate the transformed data for accuracy and consistency. Create data models and define relationships between datasets for a more unified data model. Build dashboards and reports in the dashboards section. Be sure to regularly monitor the performance of the data integration and reporting processes.\"\n",
    "input_tutorial = {\n",
    "        # 'business_problem': 'Different departments have their own databases and data sources, making it difficult to get a unified view of the business. Birst can integrate data from multiple sources to provide a single, cohesive view, enabling better decision-making.',\n",
    "        # 'tutorial_title': 'How to resolve Data Silos.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Explain specific aspects of Birst features, such as explain what the differences are between different spaces. Refrain from using 'we' for a more formal tone.\"\n",
    "    }\n",
    "training_data.append(input_tutorial)\n",
    "\n",
    "steps =  \"Set up your Birst environment and create a space that is most appropriate for your project. Connect to Data sources by clicking on the 'add new source' option in the data sources section. You can connect to databases, cloud services, or other files. Go to the data preparation section and perform data cleansing actions. Apply transformations to normalize the data and validate the transformed data for accuracy and consistency. Create data models and define relationships between datasets for a more unified data model. Set up self-service portals, provide training sessions and resources, and create templates for reports and dashboards. Build reports and dashboards within your space. Share them with relevant stakeholders via links and emails.\"\n",
    "input_tutorial2 = {\n",
    "        # 'business_problem': 'Generating reports and performing data analysis is time-consuming and often requires IT intervention. Birst offers a self-service BI platform, allowing users to create reports and dashboards quickly without relying on IT.',\n",
    "        # 'tutorial_title': 'Improving slow reporting and analysis.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"The steps maintain a professional tone. Steps are too vague. Provide explanations for why the user has to perform these actions. Define the specifications of Birst features.\"\n",
    "    }\n",
    "training_data.append(input_tutorial2)\n",
    "\n",
    "steps = \"Set up your Birst environment and create a space that is most appropriate for your project. Connect to Data sources by clicking on the 'add new source' option in the data sources section. You can connect to databases, cloud services, or other files. Initiate data extraction from each connected source. Go to the data preparation section and perform data cleansing actions. Apply transformations to normalize the data and validate the transformed data for accuracy and consistency. Set up validation rules to ensure data integrity and accuracy and execute them. Create data models and define relationships. Build reports and dashboards within your space. Regularly monitor the data quality to ensure it remains accurate.\"\n",
    "input_tutorial3 = {\n",
    "        # 'business_problem': 'Inconsistent and inaccurate data leading to poor business decisions. Birsts data preparation tools help clean and standardize data, ensuring that decisions are based on accurate and reliable information.',\n",
    "        # 'tutorial_title': 'Decreasing inaccurate data with Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"The steps maintain a professional tone. Steps are too vague. Provide explanations for why the user has to perform these actions. Define the specifications of Birst features.\"\n",
    "    }\n",
    "training_data.append(input_tutorial3)\n",
    "\n",
    "steps = \"Connect to real-time data sources. Use Birst's data connectors to integrate real-time data with existing datasets. Create dashboards that visualize the data with live updates. Set up alerts and notifications to inform stakeholders of important real-time changes. Continuously monitor the performance of real-time data streams and optimize as needed.\"\n",
    "input_tutorial4 = {\n",
    "        # 'business_problem': 'Decisions are made based on outdated information. Birst can provide real-time analytics and dashboards, allowing businesses to make timely decisions based on the most current data available.',\n",
    "        # 'tutorial_title': 'How to view real-time insights in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Steps should also include opening the Birst application and how to set up a workspace.\"\n",
    "    }\n",
    "training_data.append(input_tutorial4)\n",
    "\n",
    "steps = \"Establish clear data governance policies and document them. Use Birst's data governance tools to enforce access controls, data lineage, and data quality rules. Assign roles and permissions. Set up auditing and logging mechanisms to track data access and changes. Educate users on data governance policies.\"\n",
    "input_tutorial5 = {\n",
    "        # 'business_problem': 'Lack of data governance leading to compliance issues and data misuse. Birst includes data governance features to ensure that data is managed, controlled, and used correctly.',\n",
    "        # 'tutorial_title': 'Managining poor data governance in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Specify where the user should establish clear data governance policies.\"\n",
    "    }\n",
    "training_data.append(input_tutorial5)\n",
    "\n",
    "steps = \"Connect and import data from various sources. Use Birst's automated data modeling tools to create initial data models. Make manual adjustments to the data model as needed to fit specific business requirements. Establish relationships between tables and datasets. Validate the data models to ensure accuracy.\"\n",
    "input_tutorial6 = {\n",
    "        # 'business_problem': \"Difficulty in creating and maintaining complex data models. Birst's automated data modeling capabilities simplify the process, making it easier to create and manage data models. Explain how the user can set up auditing and logging mechanisms.\",\n",
    "        # 'tutorial_title': 'How to use Birst for complex data modeling.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Do not skip over steps in between, such as creating a work space when the user opens the Birst application.\"\n",
    "    }\n",
    "training_data.append(input_tutorial6)\n",
    "\n",
    "steps = \"Collaborate with stakeholders to identify key performance indicators relevant to the business. Define the metrics and data sources needed to calculate these indicators. Build customizable dashboards to visualize and track them. Establish targets and benchmarks for each KPI. Regularly review the dashboards to monitor performance.\"\n",
    "input_tutorial7 = {\n",
    "        # 'business_problem': 'Inability to track key performance indicators (KPIs) effectively. Birst allows businesses to define, track, and visualize KPIs through customizable dashboards and reports.',\n",
    "        # 'tutorial_title': 'Improving ineffective performance metrics in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Do not skip over steps in between, such as creating a work space when the user opens the Birst application. Specify where the user can define the metrics and how to establish targets and benchmarks.\"\n",
    "    }\n",
    "training_data.append(input_tutorial7)\n",
    "\n",
    "steps = \"Connect to various customer data sources. Use Birst to unify customer data into a single dataset. Create segments based on customer behavior and preferences. Use the insights to drive marketing sales and customer service strategies.\"\n",
    "input_tutorial8 = {\n",
    "        # 'business_problem': 'Incomplete view of customer behavior and preferences. Birst can integrate customer data from various channels to provide a comprehensive view, aiding in better customer relationship management.',\n",
    "        # 'tutorial_title': 'How to integrate customer data in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Do not skip over steps in between, such as creating a work space when the user opens the Birst application. Specify where the user can unify data and create segments in Birst.\"\n",
    "    }\n",
    "training_data.append(input_tutorial8)\n",
    "\n",
    "steps = \"Enable Birst's collaboration features. Create and share dashboards and reports with team members. Use annotations and comments to discuss insights and share feedback. Implement version control to manage changes and updates to reports and dashboards. Provide training on collaboration tools and best practices.\"\n",
    "input_tutorial9 = {\n",
    "        # 'business_problem': 'Difficulty in sharing insights and collaborating on data analysis across teams. Birst supports collaborative analytics, allowing team members to share reports, dashboards, and insights easily.',\n",
    "        # 'tutorial_title': 'How to collaborate in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Do not skip over steps in between, such as creating a work space when the user opens the Birst application. Specify how the user can use annotations and how to implement version control.\"\n",
    "    }\n",
    "training_data.append(input_tutorial9)\n",
    "\n",
    "steps = \"Assess the current and future scalability needs of the business. Implement Birst in a cloud environment to leverage its scalable infrastrucutre. Monitor and manage cloud resources to ensure they are allocated efficiently. Set up automated scaling rules to handle increased data volumes and user numbers. Continuously monitor performance and optimize as needed.\"\n",
    "input_tutorial10 = {\n",
    "        # 'business_problem': \"Existing business intelligence infrastructure can't keep up with the business growth. Birst is a scalable solution that can grow with the business, handling increasing data volumes and user numbers efficiently.\",\n",
    "        # 'tutorial_title': 'How to scale a solution in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Do not skip over steps in between, such as creating a work space when the user opens the Birst application. Specify how the user can implement Birst in a cloud environment.\"\n",
    "    }\n",
    "training_data.append(input_tutorial10)\n",
    "\n",
    "steps = \"Collaborate with stakeholders to identify areas where predictive analytics can add value. Gather historical data required for predictive modeling. Integrate Birst with advanced analytics tools. Develop predictive models using historical data and advanced analytics tools. Create dashboards to visualize future trends and forecasts. Continuously refine predictive models based on new data and feedback.\"\n",
    "input_tutorial11 = {\n",
    "        # 'business_problem': 'Inability to forecast future trends and performance. Birst integrates with advanced analytics tools to provide predictive analytics capabilities, helping businesses anticipate trends and plan accordingly.',\n",
    "        # 'tutorial_title': 'How to make predictions for future trends and performance in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Steps are too vague, explain why the user has to perform these steps and what features to utilize. Do not skip over steps in between, such as creating a work space when the user opens the Birst application. Define specific criteria for historical data and how to develop predictive models using that data. Explain how to create dashboards with more detail and how to refine the models.\"\n",
    "    }\n",
    "training_data.append(input_tutorial11)\n",
    "\n",
    "steps = \"Emphasize Birst's user-friendly during training sessions. Provide comprehensive training on Birst's self-service capabilities. Develop onboarding programs to assist new users in getting started with Birst. Keep users informed about new features and updates. Offer ongoing support resources to help users navigate and use the business intelligence tools effectively.\"\n",
    "input_tutorial12 = {\n",
    "        # 'business_problem': \"Low adoption rates of business intelligence tools among employees. Birst's user-friendly interface and self-service capabilities can drive higher adoption rates among non-technical users.\",\n",
    "        # 'tutorial_title': \"Using Birst's user-friendly interface.\",\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Define what onboarding programs will consist of. Provide examples for how the user can discuss new features and updates.\"\n",
    "    }\n",
    "training_data.append(input_tutorial12)\n",
    "\n",
    "steps = \"Conduct a security assessment to identify potential risks and compliance requirements. Implement robust access controls to restrict data access to authorized users only. Enable data encryption for data at rest and in transit. Configure Birst to comply with relevant industry standards and regulations. Set up audit logs to track data access and modifications for compliance purposes. Provide security training to users to ensure they understand best practices and compliance requirements.\"\n",
    "input_tutorial13 = {\n",
    "        # 'business_problem': 'Difficulty in ensuring data security and compliance with regulations. Birst offers robust security features and compliance controls to protect sensitive data and comply with industry standards.',\n",
    "        # 'tutorial_title': 'Analyzing the supply chain in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Explain what does a security assessment look like. Also define how to use Birst to unify supply chain data into a single dataset. Specify where the user can identify key supply chain metrics to monitor in Birst and what analytical tools that can be used.\"\n",
    "    }\n",
    "training_data.append(input_tutorial13)\n",
    "\n",
    "steps = \"Connect to supply chain data sources. Use Birst to unify supply chain data into a single dataset. Identify key supply chain metrics to monitor. Create dashboards to visualize supply chain performance and track key metrics. Use Birst's analytical tools to identify bottlenecks and inefficiencies. Implement data-driven strategies to optimize suplly chain processes based on insights gained from Birst.\"\n",
    "input_tutorial14 = {\n",
    "        # 'business_problem': \"Lack of visibility and inefficiencies in the supply chain. Birst can analyze supply chain data to identify bottlenecks, optimize processes, and improve overall efficiency.\",\n",
    "        # 'tutorial_title': 'How to view real-time insights in Birst.',\n",
    "        'tutorial_steps': sent_tokenize(steps),\n",
    "        'feedback': \"Explain in detail how the user can connect to a supply chain data sources and what analytical tools that Birst has to offer. Also provide more instructions for how to use those tools in Birst. Define what bottlenecks are and how they affect the supply chain.\"\n",
    "    }\n",
    "training_data.append(input_tutorial14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b121b3-ca01-4469-9158-229833e2c1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526405ef59c241c5bd3cc913ebc202d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9bd09fe1c24e46983dd8c75027e835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 36:25, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>14.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>10.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.826800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.748400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=6.80167932510376, metrics={'train_runtime': 2235.1671, 'train_samples_per_second': 0.447, 'train_steps_per_second': 0.022, 'total_flos': 135341801472000.0, 'train_loss': 6.80167932510376, 'epoch': 50.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR='results_t5small'\n",
    "# model_checkpoint = \"t5-small\"\n",
    "# model2 = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "# tokenizer2 = T5Tokenizer.from_pretrained(model_checkpoint, legacy=False)\n",
    "\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "# 90% for training data and 10% for validation\n",
    "train_data, validation_data = train_test_split(training_data, test_size=0.1)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_list(train_data),\n",
    "    'validation': Dataset.from_list(validation_data),\n",
    "})\n",
    "\n",
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=50,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=5, \n",
    "    save_steps=10_000,\n",
    "    save_total_limit=3\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation']\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2526930a-bf29-4cfa-ad35-b87be707663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborate with stakeholders to identify areas where predictive analytics can add value. Gather historical data required for predictive modeling. Create dashboards to visualize future trends and forecasts. Continuously refine predictive models based on new data and feedback.\n"
     ]
    }
   ],
   "source": [
    "# Sample tutorial\n",
    "problem = \"Using a professional space gives business users access to Birst's self-service data preparation interface.\"\n",
    "title = \"How to use a professional space\"\n",
    "steps = \"Collaborate with stakeholders to identify areas where predictive analytics can add value. Gather historical data required for predictive modeling. Integrate Birst with advanced analytics tools. Develop predictive models using historical data and advanced analytics tools. Create dashboards to visualize future trends and forecasts. Continuously refine predictive models based on new data and feedback.\"\n",
    "\n",
    "custom_prefix = \"Review the tutorial:\"\n",
    "input_text = f\"{custom_prefix} {steps}\"\n",
    "\n",
    "# inputs = preprocess_input(problem, title, steps)\n",
    "inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "# Get output (feedback)\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=150, \n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Print output if it does not match the given input\n",
    "for o in outputs:\n",
    "    generated_sent = tokenizer.decode(o, skip_special_tokens=True)\n",
    "\n",
    "    if (generated_sent.lower() != steps.lower()):\n",
    "        print(generated_sent)\n",
    "        \n",
    "# generated_text = tokenizer2.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cef4506c-400a-461b-bcc6-9a8404d1c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def get_output():\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    steps = \"We can collaborate with stakeholders to identify areas where predictive analytics can add value.\"\n",
    "    sent_res = sentiment.polarity_scores(steps)\n",
    "\n",
    "    result_str = \"\"\n",
    "    \n",
    "    if \"we\" in steps.lower().split():\n",
    "        result_str += \"Refrain from using terms, such as 'we' and 'our'.\"\n",
    "        result_str += \" \"\n",
    "        \n",
    "    if sent_res['neu'] == 1.0 or sent_res['neu'] >= 0.8:\n",
    "        result_str +=\"Tutorial maintains a professional tone and clarity.\"\n",
    "        result_str += \" \"\n",
    "        \n",
    "    else:\n",
    "        result_str += \"Tutorial should be written in a neutral tone to maintain professionalism.\"\n",
    "        result_str += \" \"\n",
    "\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0609b6b7-9213-4751-b589-332f96ed224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tk.Tk()\n",
    "m.title('Infor Developer Assistant (InDevA)')\n",
    "m.geometry(\"600x400\")\n",
    "\n",
    "# Stores input tutorial\n",
    "input = tk.StringVar() \n",
    "\n",
    "# Sentiment Analysis\n",
    "def get_output(steps):\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    sent_res = sentiment.polarity_scores(steps)\n",
    "\n",
    "    result_str = \"\"\n",
    "    \n",
    "    if \"we\" in steps.lower().split():\n",
    "        result_str += \"Refrain from using terms, such as 'we' and 'our'.\"\n",
    "        result_str += \" \"\n",
    "        \n",
    "    if sent_res['neu'] == 1.0 or sent_res['neu'] >= 0.8:\n",
    "        result_str +=\"Tutorial maintains a professional tone and clarity.\"\n",
    "        result_str += \" \"\n",
    "        \n",
    "    else:\n",
    "        result_str += \"Tutorial should be written in a neutral tone to maintain professionalism.\"\n",
    "        result_str += \" \"\n",
    "\n",
    "    return result_str\n",
    "\n",
    "def submit():\n",
    "    steps = input.get()\n",
    "    result = get_output(steps)\n",
    "    messagebox.showinfo(\"Result\", result)\n",
    "    \n",
    "# Input label\n",
    "input_label = tk.Label(m, text='Tutorial Input').grid(row=0)\n",
    "# Entry box\n",
    "entry = tk.Entry(m, textvariable=input, width=75)\n",
    "entry.grid(row=0, column=1)\n",
    "# Submit button\n",
    "sub_bttn = tk.Button(m, text='Enter', command=submit)\n",
    "sub_bttn.grid(row=1, column=1)\n",
    "m.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a0dc9-1e13-44f8-8610-e602bad59ca8",
   "metadata": {},
   "source": [
    "# DEMO INPUT\n",
    "We can collaborate with stakeholders to identify areas where predictive analytics can add value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6460386-3c4a-44b3-9325-920e16ad3c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d457646-2379-43ae-8d17-ce53e6320d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29cf4c-e310-402e-ad8b-883a4fe21d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22119069-dc6b-4c7f-8b9d-8c2b1f47a5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbf380-110f-41c8-a154-89009254b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b450ab-afff-4463-af2c-7f492d69d046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
